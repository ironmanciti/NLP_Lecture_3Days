{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e88b86f",
   "metadata": {
    "id": "1e88b86f"
   },
   "source": [
    "# RAG (Retrieval Augmented Generation) 실습\n",
    "\n",
    "RAG는 LLM이 내부 지식만으로 답하는 방식이 아니라, 질문과 관련된 문서를 추가로 제공받아 그것을 근거로 답변을 생성하는 방법입니다.\n",
    "\n",
    "## RAG Flow\n",
    "1. **Document Ingestion**: 문서를 작은 청크(chunk)로 분할하고 임베딩으로 변환하여 벡터 저장소에 저장\n",
    "2. **Query Processing**: 사용자 질문을 임베딩으로 변환\n",
    "3. **Retrieval**: 질문 임베딩과 유사한 문서 청크를 검색\n",
    "4. **Generation**: 검색된 문서와 질문을 함께 LLM에 제공하여 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51746481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수 로드\n",
    "# Gemini API 클라이언트 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045912f",
   "metadata": {
    "id": "b045912f"
   },
   "source": [
    "## 1. Source: 문서 데이터 준비\n",
    "\n",
    "RAG 시스템의 첫 단계는 다양한 소스에서 문서를 준비하는 것입니다.\n",
    "실제 환경에서는 PDF, 웹사이트, 데이터베이스 등에서 문서를 로드합니다.\n",
    "Text, PPT, Image, PDF, HTML 등 다양한 비정형 데이터 소스를 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 문서 데이터 (실제로는 외부 소스에서 로드)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2b367",
   "metadata": {
    "id": "dce2b367"
   },
   "source": [
    "## 2. Load & Transform: 문서를 청크로 분할\n",
    "\n",
    "다양한 소스(Web Site, DB, YouTube 등)에서 문서(HTML, PDF, JSON, Word, PPT, 코드 등)를 로드합니다.\n",
    "긴 문서는 작은 단위(청크)로 나누어야 합니다. 이렇게 하면:\n",
    "- 임베딩 생성이 효율적입니다\n",
    "- 검색 시 더 정확한 관련 부분을 찾을 수 있습니다\n",
    "- LLM의 컨텍스트 길이 제한을 고려할 수 있습니다\n",
    "\n",
    "데이터 변환 및 정제를 위한 여러 변환 단계를 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text, chunk_size=200, overlap=50):\n",
    "# 모든 문서를 청크로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45831e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e6e963",
   "metadata": {
    "id": "51e6e963"
   },
   "source": [
    "## 3. Embed: 문서를 임베딩으로 변환\n",
    "\n",
    "각 문서 청크를 숫자 벡터(임베딩)로 변환합니다.\n",
    "유사한 의미를 가진 텍스트는 유사한 벡터로 표현됩니다.\n",
    "문서에 대한 임베딩을 만들어 유사한 다른 텍스트 부분을 빠르고 효율적으로 검색할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eac4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts, model=\"gemini-embedding-001\", task_type=\"RETRIEVAL_DOCUMENT\"):\n",
    "    # 각 임베딩을 numpy 배열로 변환\n",
    "# 모든 청크를 임베딩으로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6d4b5",
   "metadata": {
    "id": "88a6d4b5"
   },
   "source": [
    "## 4. Store: 임베딩을 벡터 저장소에 저장\n",
    "\n",
    "생성된 임베딩을 벡터 저장소에 저장합니다.\n",
    "실제 환경에서는 Pinecone, Weaviate, ChromaDB 등의 벡터 데이터베이스를 사용합니다.\n",
    "임베딩의 효율적인 저장 및 검색을 지원하는 데이터베이스입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 저장소 (실제로는 벡터 DB를 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3b019",
   "metadata": {
    "id": "2be3b019"
   },
   "source": [
    "## 5. Retrieve: 질문과 유사한 문서 검색\n",
    "\n",
    "사용자 질문을 임베딩으로 변환하고, 저장된 문서 임베딩과의 유사도를 계산하여\n",
    "가장 관련성 높은 문서를 검색합니다.\n",
    "검색 알고리즘을 통해 문서 유사도를 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_chunks(query, vector_store, top_k=3):\n",
    "    # 1. 질문을 임베딩으로 변환\n",
    "    # 2. 모든 청크와의 유사도 계산\n",
    "    # 3. 상위 k개 선택\n",
    "    # 4. 결과 반환\n",
    "# 검색 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955d395",
   "metadata": {
    "id": "7955d395"
   },
   "source": [
    "## 6. RAG 시스템 구현\n",
    "\n",
    "검색된 문서를 컨텍스트로 사용하여 Gemini API로 답변을 생성합니다.\n",
    "이것이 RAG의 핵심입니다: 검색(Retrieval) + 생성(Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c535601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, vector_store, top_k=3):\n",
    "    # 1. 관련 문서 검색\n",
    "    # 2. 컨텍스트 구성\n",
    "    # 3. 프롬프트 구성\n",
    "    # 4. Gemini API로 답변 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbb952",
   "metadata": {
    "id": "46bbb952"
   },
   "source": [
    "## 7. RAG 시스템 테스트\n",
    "\n",
    "다양한 질문으로 RAG 시스템을 테스트해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e928a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 쿼리들\n",
    "    # RAG로 답변 생성\n",
    "    # 검색된 문서 출력\n",
    "    # 생성된 답변 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009fa8f",
   "metadata": {
    "id": "0009fa8f"
   },
   "source": [
    "## 8. RAG vs 일반 LLM 비교\n",
    "\n",
    "RAG를 사용하면 LLM이 최신 정보나 특정 도메인 지식을 활용할 수 있습니다.\n",
    "일반 LLM과 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0926ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a03ac0",
   "metadata": {
    "id": "d1a03ac0"
   },
   "source": [
    "## 10. 실습: RAG 시스템 개선하기\n",
    "\n",
    "위의 코드를 수정하여 다음을 시도해보세요:\n",
    "1. 더 많은 문서를 추가해보기\n",
    "2. 청크 크기와 overlap을 조정해보기\n",
    "3. 검색할 상위 k개 문서 수를 변경해보기\n",
    "4. 프롬프트를 개선하여 더 나은 답변 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 공간\n",
    "# 여기서 자유롭게 실험해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22b572",
   "metadata": {
    "id": "6a22b572"
   },
   "source": [
    "## 요약\n",
    "\n",
    "RAG 시스템은 다음 단계로 구성됩니다:\n",
    "\n",
    "1. **Source**: 다양한 소스에서 문서 수집 (Text, PPT, Image, PDF, HTML 등)\n",
    "2. **Load**: 다양한 소스에서 문서 로드 (Web Site, DB, YouTube 등)\n",
    "3. **Transform**: 문서를 적절한 크기의 청크로 분할 (데이터 변환 및 정제)\n",
    "4. **Embed**: 각 청크를 임베딩 벡터로 변환 (유사한 텍스트 부분을 빠르고 효율적으로 검색)\n",
    "5. **Store**: 임베딩을 벡터 저장소에 저장 (효율적인 저장 및 검색을 지원하는 데이터베이스)\n",
    "6. **Retrieve**: 질문과 유사한 문서를 검색 (검색 알고리즘을 통한 문서 유사도 측정)\n",
    "7. **Prompt**: 검색된 문서와 질문을 결합하여 프롬프트 생성\n",
    "8. **LLM**: LLM으로 답변 생성\n",
    "9. **Answer**: 최종 답변 반환\n",
    "\n",
    "이 과정을 통해 LLM은 최신 정보와 특정 도메인 지식을 활용하여 더 정확한 답변을 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cefac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
