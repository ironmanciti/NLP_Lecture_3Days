{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e88b86f",
      "metadata": {
        "id": "1e88b86f"
      },
      "source": [
        "# RAG (Retrieval Augmented Generation) 실습\n",
        "\n",
        "RAG는 LLM이 내부 지식만으로 답하는 방식이 아니라, 질문과 관련된 문서를 추가로 제공받아 그것을 근거로 답변을 생성하는 방법입니다.\n",
        "\n",
        "## RAG Flow\n",
        "1. **Document Ingestion**: 문서를 작은 청크(chunk)로 분할하고 임베딩으로 변환하여 벡터 저장소에 저장\n",
        "2. **Query Processing**: 사용자 질문을 임베딩으로 변환\n",
        "3. **Retrieval**: 질문 임베딩과 유사한 문서 청크를 검색\n",
        "4. **Generation**: 검색된 문서와 질문을 함께 LLM에 제공하여 답변 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1de3830c",
      "metadata": {
        "id": "1de3830c"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 환경 변수 로드\n",
        "load_dotenv()\n",
        "\n",
        "# Gemini API 클라이언트 초기화\n",
        "client = genai.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b045912f",
      "metadata": {
        "id": "b045912f"
      },
      "source": [
        "## 1. Source: 문서 데이터 준비\n",
        "\n",
        "RAG 시스템의 첫 단계는 다양한 소스에서 문서를 준비하는 것입니다.\n",
        "실제 환경에서는 PDF, 웹사이트, 데이터베이스 등에서 문서를 로드합니다.\n",
        "Text, PPT, Image, PDF, HTML 등 다양한 비정형 데이터 소스를 지원합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6662b8f6",
      "metadata": {
        "id": "6662b8f6",
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750986d2-d92a-4269-f7d4-b6aeafbe6061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 5개의 문서가 준비되었습니다.\n",
            "\n",
            "문서 1: 인공지능(AI)은 인간의 진화 과정에서 자연 발생한 생물학적 지식입니다. 머신러닝과 딥러닝...\n",
            "문서 2: 자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 AI의 한 ...\n",
            "문서 3: Transformer는 2010년 영화사에서 만들어낸 인공지능 모델로, 어텐션 메커니즘을 ...\n",
            "문서 4: RAG(Retrieval Augmented Generation)는 외부 지식 베이스에서 관...\n",
            "문서 5: 벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색할 수 있는 데이터베이스입니다. ...\n"
          ]
        }
      ],
      "source": [
        "# 샘플 문서 데이터 (실제로는 외부 소스에서 로드)\n",
        "documents = [\n",
        "    \"인공지능(AI)은 인간의 진화 과정에서 자연 발생한 생물학적 지식입니다. 머신러닝과 딥러닝은 AI의 하위 분야로, 대량의 데이터를 통해 패턴을 학습합니다.\",\n",
        "    \"자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 AI의 한 분야입니다. 텍스트 분석, 번역, 감성 분석, 챗봇 등 다양한 응용 분야가 있습니다.\",\n",
        "    \"Transformer는 2010년 영화사에서 만들어낸 인공지능 모델로, 어텐션 메커니즘을 핵심으로 합니다. BERT, GPT 등 최신 언어 모델의 기반이 되었습니다.\",\n",
        "    \"RAG(Retrieval Augmented Generation)는 외부 지식 베이스에서 관련 정보를 검색하여 LLM의 답변을 보강하는 기법입니다. 이를 통해 모델의 최신 정보 접근과 정확도가 향상됩니다.\",\n",
        "    \"벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색할 수 있는 데이터베이스입니다. 임베딩 벡터를 저장하고 유사도 검색에 활용됩니다.\"\n",
        "]\n",
        "\n",
        "print(f\"총 {len(documents)}개의 문서가 준비되었습니다.\\n\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"문서 {i}: {doc[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dce2b367",
      "metadata": {
        "id": "dce2b367"
      },
      "source": [
        "## 2. Load & Transform: 문서를 청크로 분할\n",
        "\n",
        "다양한 소스(Web Site, DB, YouTube 등)에서 문서(HTML, PDF, JSON, Word, PPT, 코드 등)를 로드합니다.\n",
        "긴 문서는 작은 단위(청크)로 나누어야 합니다. 이렇게 하면:\n",
        "- 임베딩 생성이 효율적입니다\n",
        "- 검색 시 더 정확한 관련 부분을 찾을 수 있습니다\n",
        "- LLM의 컨텍스트 길이 제한을 고려할 수 있습니다\n",
        "\n",
        "데이터 변환 및 정제를 위한 여러 변환 단계를 포함합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "204abe7d",
      "metadata": {
        "id": "204abe7d",
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aff209f-7726-44f0-d9fd-27e850a28fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1이 1개의 청크로 분할되었습니다.\n",
            "문서 2이 1개의 청크로 분할되었습니다.\n",
            "문서 3이 1개의 청크로 분할되었습니다.\n",
            "문서 4이 1개의 청크로 분할되었습니다.\n",
            "문서 5이 1개의 청크로 분할되었습니다.\n",
            "\n",
            "총 5개의 청크가 생성되었습니다.\n"
          ]
        }
      ],
      "source": [
        "def split_into_chunks(text, chunk_size=200, overlap=50):\n",
        "    \"\"\"\n",
        "    긴 텍스트를 청크로 분할\n",
        "\n",
        "    Args:\n",
        "        text: 분할할 텍스트\n",
        "        chunk_size: 각 청크의 크기 (문자 수)\n",
        "        overlap: 청크 간 겹치는 부분 (문자 수)\n",
        "\n",
        "    Returns:\n",
        "        list: 청크 리스트\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# 모든 문서를 청크로 분할\n",
        "all_chunks = []\n",
        "for i, doc in enumerate(documents):\n",
        "    chunks = split_into_chunks(doc, chunk_size=150, overlap=30)\n",
        "    all_chunks.extend(chunks)\n",
        "    print(f\"문서 {i+1}이 {len(chunks)}개의 청크로 분할되었습니다.\")\n",
        "\n",
        "print(f\"\\n총 {len(all_chunks)}개의 청크가 생성되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_chunks[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kbiulcqs1kDu",
        "outputId": "3715c83c-ffcb-4938-cb3a-e6e7c967bb02"
      },
      "id": "kbiulcqs1kDu",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 AI의 한 분야입니다. 텍스트 분석, 번역, 감성 분석, 챗봇 등 다양한 응용 분야가 있습니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51e6e963",
      "metadata": {
        "id": "51e6e963"
      },
      "source": [
        "## 3. Embed: 문서를 임베딩으로 변환\n",
        "\n",
        "각 문서 청크를 숫자 벡터(임베딩)로 변환합니다.\n",
        "유사한 의미를 가진 텍스트는 유사한 벡터로 표현됩니다.\n",
        "문서에 대한 임베딩을 만들어 유사한 다른 텍스트 부분을 빠르고 효율적으로 검색할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "f3689227",
      "metadata": {
        "id": "f3689227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4b18e8-ed1a-40fb-fde2-bfb95afbd8b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "청크를 임베딩으로 변환 중...\n",
            "\n",
            "임베딩 완료! Shape: (5, 3072)\n",
            "각 청크는 3072차원 벡터로 표현됩니다.\n"
          ]
        }
      ],
      "source": [
        "def embed_texts(texts, model=\"gemini-embedding-001\", task_type=\"RETRIEVAL_DOCUMENT\"):\n",
        "    \"\"\"\n",
        "    텍스트 리스트를 임베딩 벡터로 변환\n",
        "\n",
        "    Args:\n",
        "        texts: 임베딩할 텍스트 리스트\n",
        "        model: 사용할 임베딩 모델\n",
        "        task_type: 작업 유형 (\"SEMANTIC_SIMILARITY\", \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\" 등)\n",
        "\n",
        "    Returns:\n",
        "        numpy array: 임베딩 벡터 행렬\n",
        "    \"\"\"\n",
        "    result = client.models.embed_content(\n",
        "        model=model,\n",
        "        contents=texts,\n",
        "        config=types.EmbedContentConfig(task_type=task_type)\n",
        "    )\n",
        "\n",
        "    # 각 임베딩을 numpy 배열로 변환\n",
        "    embeddings = np.array([np.array(e.values) for e in result.embeddings])\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# 모든 청크를 임베딩으로 변환\n",
        "print(\"청크를 임베딩으로 변환 중...\")\n",
        "chunk_embeddings = embed_texts(all_chunks, task_type=\"RETRIEVAL_DOCUMENT\")\n",
        "\n",
        "print(f\"\\n임베딩 완료! Shape: {chunk_embeddings.shape}\")\n",
        "print(f\"각 청크는 {chunk_embeddings.shape[1]}차원 벡터로 표현됩니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a6d4b5",
      "metadata": {
        "id": "88a6d4b5"
      },
      "source": [
        "## 4. Store: 임베딩을 벡터 저장소에 저장\n",
        "\n",
        "생성된 임베딩을 벡터 저장소에 저장합니다.\n",
        "실제 환경에서는 Pinecone, Weaviate, ChromaDB 등의 벡터 데이터베이스를 사용합니다.\n",
        "임베딩의 효율적인 저장 및 검색을 지원하는 데이터베이스입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c846f76d",
      "metadata": {
        "id": "c846f76d",
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1264da06-6d93-4aed-ff05-f57875f7e00a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "벡터 저장소에 저장 완료!\n",
            "- 저장된 청크 수: 5\n",
            "- 임베딩 벡터 shape: (5, 3072)\n"
          ]
        }
      ],
      "source": [
        "# 벡터 저장소 (실제로는 벡터 DB를 사용)\n",
        "vector_store = {\n",
        "    'chunks': all_chunks,\n",
        "    'embeddings': chunk_embeddings\n",
        "}\n",
        "\n",
        "print(\"벡터 저장소에 저장 완료!\")\n",
        "print(f\"- 저장된 청크 수: {len(vector_store['chunks'])}\")\n",
        "print(f\"- 임베딩 벡터 shape: {vector_store['embeddings'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be3b019",
      "metadata": {
        "id": "2be3b019"
      },
      "source": [
        "## 5. Retrieve: 질문과 유사한 문서 검색\n",
        "\n",
        "사용자 질문을 임베딩으로 변환하고, 저장된 문서 임베딩과의 유사도를 계산하여\n",
        "가장 관련성 높은 문서를 검색합니다.\n",
        "검색 알고리즘을 통해 문서 유사도를 측정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "f5960f90",
      "metadata": {
        "id": "f5960f90",
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749304ca-e0ca-4e8c-c5f8-4ef0b2242a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문: 인공지능이란 무엇인가요?\n",
            "\n",
            "검색 결과:\n",
            "1. (유사도: 0.7400) 인공지능(AI)은 인간의 진화 과정에서 자연 발생한 생물학적 지식입니다. 머신러닝과 딥러닝은 AI의 하위 분야로, 대량의 데이터를 통해 패턴을 학습합니다.\n",
            "2. (유사도: 0.6642) 자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 AI의 한 분야입니다. 텍스트 분석, 번역, 감성 분석, 챗봇 등 다양한 응용 분야가 있습니다.\n",
            "3. (유사도: 0.6460) Transformer는 2010년 영화사에서 만들어낸 인공지능 모델로, 어텐션 메커니즘을 핵심으로 합니다. BERT, GPT 등 최신 언어 모델의 기반이 되었습니다.\n"
          ]
        }
      ],
      "source": [
        "def search_relevant_chunks(query, vector_store, top_k=3):\n",
        "    \"\"\"\n",
        "    질문과 가장 유사한 청크를 검색\n",
        "\n",
        "    Args:\n",
        "        query: 사용자 질문\n",
        "        vector_store: 벡터 저장소 (chunks, embeddings 포함)\n",
        "        top_k: 반환할 상위 청크 개수\n",
        "\n",
        "    Returns:\n",
        "        list: (유사도, 청크) 튜플 리스트\n",
        "    \"\"\"\n",
        "    # 1. 질문을 임베딩으로 변환\n",
        "    query_embedding = embed_texts(\n",
        "        [query],\n",
        "        task_type=\"RETRIEVAL_QUERY\"\n",
        "    )[0]\n",
        "\n",
        "    # 2. 모든 청크와의 유사도 계산\n",
        "    similarities = cosine_similarity([query_embedding], vector_store['embeddings'])[0]\n",
        "\n",
        "    # 3. 상위 k개 선택\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    # 4. 결과 반환\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append((similarities[idx], vector_store['chunks'][idx]))\n",
        "\n",
        "    return results\n",
        "\n",
        "# 검색 테스트\n",
        "test_query = \"인공지능이란 무엇인가요?\"\n",
        "print(f\"질문: {test_query}\\n\")\n",
        "print(\"검색 결과:\")\n",
        "relevant_chunks = search_relevant_chunks(test_query, vector_store, top_k=3)\n",
        "for i, (score, chunk) in enumerate(relevant_chunks, 1):\n",
        "    print(f\"{i}. (유사도: {score:.4f}) {chunk}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7955d395",
      "metadata": {
        "id": "7955d395"
      },
      "source": [
        "## 6. RAG 시스템 구현\n",
        "\n",
        "검색된 문서를 컨텍스트로 사용하여 Gemini API로 답변을 생성합니다.\n",
        "이것이 RAG의 핵심입니다: 검색(Retrieval) + 생성(Generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "67ce37b8",
      "metadata": {
        "id": "67ce37b8",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def rag_query(query, vector_store, top_k=3):\n",
        "    \"\"\"\n",
        "    RAG를 사용하여 쿼리에 대한 답변 생성\n",
        "\n",
        "    Args:\n",
        "        query: 사용자 질문\n",
        "        vector_store: 벡터 저장소\n",
        "        top_k: 검색할 상위 청크 개수\n",
        "\n",
        "    Returns:\n",
        "        tuple: (생성된 답변, 관련 청크 리스트)\n",
        "    \"\"\"\n",
        "    # 1. 관련 문서 검색\n",
        "    relevant_chunks = search_relevant_chunks(query, vector_store, top_k)\n",
        "\n",
        "    # 2. 컨텍스트 구성\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"[참고 문서 {i+1}] {chunk}\"\n",
        "        for i, (score, chunk) in enumerate(relevant_chunks)\n",
        "    ])\n",
        "\n",
        "    # 3. 프롬프트 구성\n",
        "    prompt = f\"\"\"다음 문서들을 참고하여 질문에 답변해주세요. 기존에 학습된 지식 보다 참고 문서를 우선해 주세요.\n",
        "\n",
        "참고 문서:\n",
        "{context}\n",
        "\n",
        "질문: {query}\n",
        "\n",
        "답변:\"\"\"\n",
        "\n",
        "    # 4. Gemini API로 답변 생성\n",
        "    model = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    return model.text, relevant_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46bbb952",
      "metadata": {
        "id": "46bbb952"
      },
      "source": [
        "## 7. RAG 시스템 테스트\n",
        "\n",
        "다양한 질문으로 RAG 시스템을 테스트해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "258927cd",
      "metadata": {
        "id": "258927cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4737ec2d-53cb-4683-9493-4d6e4fb6a18d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "질문: 인공지능이란 무엇인가요?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[참고 문서]\n",
            "1. (유사도: 0.7400) 인공지능(AI)은 인간의 진화 과정에서 자연 발생한 생물학적 지식입니다. 머신러닝과 딥러닝은 AI의 하위 분야로, 대량의 데이터를 통해 패턴을 학습합니다....\n",
            "2. (유사도: 0.6642) 자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 AI의 한 분야입니다. 텍스트 분석, 번역, 감성 분석, 챗봇 등 다양한 응용 분야가 있습니다....\n",
            "\n",
            "[답변]\n",
            "인공지능(AI)은 인간의 진화 과정에서 자연 발생한 생물학적 지식입니다.\n",
            "\n",
            "================================================================================\n",
            "질문: RAG는 어떻게 작동하나요?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[참고 문서]\n",
            "1. (유사도: 0.7442) RAG(Retrieval Augmented Generation)는 외부 지식 베이스에서 관련 정보를 검색하여 LLM의 답변을 보강하는 기법입니다. 이를 통해 모델의 최신 정보 접근...\n",
            "2. (유사도: 0.5904) Transformer는 2010년 영화사에서 만들어낸 인공지능 모델로, 어텐션 메커니즘을 핵심으로 합니다. BERT, GPT 등 최신 언어 모델의 기반이 되었습니다....\n",
            "\n",
            "[답변]\n",
            "RAG는 외부 지식 베이스에서 관련 정보를 검색하여 LLM(대규모 언어 모델)의 답변을 보강하는 방식으로 작동합니다. 이를 통해 모델의 최신 정보 접근과 정확도가 향상됩니다.\n",
            "\n",
            "================================================================================\n",
            "질문: Transformer 모델에 대해 설명해주세요.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[참고 문서]\n",
            "1. (유사도: 0.7790) Transformer는 2010년 영화사에서 만들어낸 인공지능 모델로, 어텐션 메커니즘을 핵심으로 합니다. BERT, GPT 등 최신 언어 모델의 기반이 되었습니다....\n",
            "2. (유사도: 0.6255) 자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 AI의 한 분야입니다. 텍스트 분석, 번역, 감성 분석, 챗봇 등 다양한 응용 분야가 있습니다....\n",
            "\n",
            "[답변]\n",
            "참고 문서 1에 따르면, Transformer는 2010년 영화사에서 만들어낸 인공지능 모델입니다. 이 모델은 어텐션 메커니즘을 핵심으로 하며, BERT, GPT 등 최신 언어 모델의 기반이 되었습니다.\n",
            "\n",
            "================================================================================\n",
            "질문: 벡터 데이터베이스는 무엇인가요?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[참고 문서]\n",
            "1. (유사도: 0.7853) 벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색할 수 있는 데이터베이스입니다. 임베딩 벡터를 저장하고 유사도 검색에 활용됩니다....\n",
            "2. (유사도: 0.5857) RAG(Retrieval Augmented Generation)는 외부 지식 베이스에서 관련 정보를 검색하여 LLM의 답변을 보강하는 기법입니다. 이를 통해 모델의 최신 정보 접근...\n",
            "\n",
            "[답변]\n",
            "벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색할 수 있는 데이터베이스입니다. 임베딩 벡터를 저장하고 유사도 검색에 활용됩니다.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 테스트 쿼리들\n",
        "test_queries = [\n",
        "    \"인공지능이란 무엇인가요?\",\n",
        "    \"RAG는 어떻게 작동하나요?\",\n",
        "    \"Transformer 모델에 대해 설명해주세요.\",\n",
        "    \"벡터 데이터베이스는 무엇인가요?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"질문: {query}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # RAG로 답변 생성\n",
        "    answer, relevant_chunks = rag_query(query, vector_store, top_k=2)\n",
        "\n",
        "    # 검색된 문서 출력\n",
        "    print(\"\\n[참고 문서]\")\n",
        "    for i, (score, chunk) in enumerate(relevant_chunks, 1):\n",
        "        print(f\"{i}. (유사도: {score:.4f}) {chunk[:100]}...\")\n",
        "\n",
        "    # 생성된 답변 출력\n",
        "    print(f\"\\n[답변]\")\n",
        "    print(answer)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0009fa8f",
      "metadata": {
        "id": "0009fa8f"
      },
      "source": [
        "## 8. RAG vs 일반 LLM 비교\n",
        "\n",
        "RAG를 사용하면 LLM이 최신 정보나 특정 도메인 지식을 활용할 수 있습니다.\n",
        "일반 LLM과 비교해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d4298846",
      "metadata": {
        "id": "d4298846",
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd35872-5d58-4d2d-e40e-7fa10c6c563f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "일반 LLM (RAG 없이)\n",
            "================================================================================\n",
            "Transformer(트랜스포머) 모델은 Google Brain 팀이 2017년 논문 \"Attention Is All You Need\"에서 제안한 혁신적인 신경망 아키텍처입니다. 특히 자연어 처리(NLP) 분야에서 막대한 영향을 미쳤으며, 현재 대부분의 최신 NLP 모델(BERT, GPT-x, T5 등)의 기반이 되고 있습니다.\n",
            "\n",
            "---\n",
            "\n",
            "### Transformer 모델의 등장 배경 및 목표\n",
            "\n",
            "기존의 순환 신경망(RNN)이나 장단기 메모리(LSTM) 같은 모델들은 시퀀스(순서) 데이터를 처리하는 데 강점을 가졌습니다. 하지만 다음과 같은 한계가 있었습니다.\n",
            "\n",
            "1.  **순차적 처리의 한계 (병렬 처리 불가):** RNN/LSTM은 이전 스텝의 계산 결과가 다음 스텝의 입력으로 들어가므로, 계산을 병렬로 처리하기 어렵습니다. 이는 학습 속도와 효율성을 저해합니다.\n",
            "2.  **장거리 의존성 문제 (Long-Term Dependency):** 시퀀스가 길어질수록 초반에 입력된 정보가 후반부에 제대로 전달되지 못하는 문제가 발생할 수 있습니다.\n",
            "3.  **병목 현상 (Bottleneck):** 인코더-디코더 구조에서 고정된 크기의 컨텍스트 벡터(context vector)가 전체 입력 시퀀스의 정보를 압축하여 전달하기 때문에, 정보 손실이 발생할 수 있습니다.\n",
            "\n",
            "Transformer는 이러한 한계들을 극복하기 위해 **어텐션(Attention) 메커니즘**을 전적으로 사용하여 시퀀스 데이터의 특징을 추출하고 처리합니다.\n",
            "\n",
            "---\n",
            "\n",
            "### Transformer 모델의 주요 특징 및 구성 요소\n",
            "\n",
            "Transformer 모델은 크게 **인코더(Encoder)**와 **디코더(Decoder)** 스택으로 구성됩니다. 각 스택은 여러 개의 동일한 레이어를 쌓아 올린 형태입니다.\n",
            "\n",
            "#### 1. 인코더 (Encoder)\n",
            "\n",
            "입력 시퀀스(예: 문장)를 받아 그 특징을 추출하여 컨텍스트 표현(Contextual Representation)으로 변환합니다. 각 인코더 레이어는 다음의 두 가지 서브 레이어로 구성됩니다.\n",
            "\n",
            "*   **멀티 헤드 셀프 어텐션 (Multi-Head Self-Attention):**\n",
            "    *   **셀프 어텐션 (Self-Attention)의 핵심:** 문장 내의 각 단어가 다른 단어들과 얼마나 관련되어 있는지(영향을 주는지)를 파악합니다. 예를 들어 \"The animal didn't cross the street because it was too tired\"라는 문장에서 'it'이 'animal'을 가리키는지 'street'을 가리키는지 판단하는 데 도움을 줍니다.\n",
            "    *   이를 위해 Query (Q), Key (K), Value (V)라는 세 가지 벡터를 사용합니다. 단어 자체를 Query로 삼아, 문장 내의 다른 모든 단어들(Key)과 비교하여 관련성 점수를 계산하고, 이 점수를 기반으로 Value 벡터들을 가중 평균하여 새로운 표현을 만듭니다.\n",
            "    *   **Q, K, V 계산 방식:** 입력 임베딩 벡터에 서로 다른 가중치 행렬을 곱하여 Query, Key, Value 벡터를 생성합니다.\n",
            "    *   **가중치 계산:** `softmax(Q * K^T / sqrt(d_k)) * V`\n",
            "    *   **멀티 헤드 (Multi-Head):** 여러 개의 셀프 어텐션 메커니즘을 병렬로 수행하여 각기 다른 관점(다양한 특징 공간)에서 정보를 학습하고 결합합니다. 이는 모델의 표현 능력을 크게 향상시킵니다.\n",
            "*   **피드 포워드 네트워크 (Feed-Forward Network):**\n",
            "    *   셀프 어텐션 레이어의 출력을 받아 각 위치(단어)별로 독립적으로 적용되는 간단한 완전 연결(fully-connected) 신경망입니다. 비선형성을 추가하고 더 복잡한 패턴을 학습합니다.\n",
            "\n",
            "#### 2. 디코더 (Decoder)\n",
            "\n",
            "인코더의 출력과 이전에 생성된 토큰들을 바탕으로 출력 시퀀스를 생성합니다. 각 디코더 레이어는 세 가지 서브 레이어로 구성됩니다.\n",
            "\n",
            "*   **마스크드 멀티 헤드 셀프 어텐션 (Masked Multi-Head Self-Attention):**\n",
            "    *   인코더의 셀프 어텐션과 유사하지만, 미래의 단어를 \"엿보는\" 것을 방지하기 위해 마스킹(Masking)을 적용합니다. 즉, 현재 예측하려는 단어 다음의 단어들에는 어텐션 스코어를 부여하지 않습니다. 이는 텍스트 생성 시 순차성을 유지하기 위함입니다.\n",
            "*   **멀티 헤드 인코더-디코더 어텐션 (Multi-Head Encoder-Decoder Attention):**\n",
            "    *   인코더의 출력(Key와 Value)과 디코더의 이전 마스크드 셀프 어텐션 출력(Query) 사이에서 어텐션을 수행합니다. 이를 통해 디코더는 인코더가 입력 시퀀스에서 추출한 정보를 필요한 만큼 가져와 사용할 수 있습니다. 이 과정이 기존 RNN/LSTM 모델의 병목 현상을 해결해줍니다.\n",
            "*   **피드 포워드 네트워크 (Feed-Forward Network):**\n",
            "    *   인코더와 동일하게 비선형성을 추가하고 복잡한 패턴을 학습합니다.\n",
            "\n",
            "#### 3. 추가적인 핵심 요소\n",
            "\n",
            "*   **위치 임베딩 (Positional Encoding):**\n",
            "    *   어텐션 메커니즘은 단어들의 순서 정보를 직접적으로 고려하지 않습니다. 이를 보완하기 위해 각 단어의 임베딩 벡터에 해당 단어의 **위치 정보**를 담은 벡터(Positional Encoding)를 더해줍니다. 일반적으로 사인(sine) 및 코사인(cosine) 함수를 사용하여 고정된 위치 임베딩을 생성하거나, 학습 가능한 임베딩을 사용합니다.\n",
            "*   **잔차 연결 (Residual Connections):**\n",
            "    *   각 서브 레이어의 입력과 출력을 더해주는 방식으로, 심층 신경망의 학습을 안정화하고 기울기 소실(vanishing gradients) 문제를 완화합니다. `Sublayer(x) + x` 형태입니다.\n",
            "*   **레이어 정규화 (Layer Normalization):**\n",
            "    *   각 서브 레이어의 출력에 적용되어 학습 과정을 안정화하고 속도를 향상시킵니다.\n",
            "\n",
            "---\n",
            "\n",
            "### Transformer 모델의 장점\n",
            "\n",
            "1.  **병렬 처리 가능:** RNN/LSTM과 달리 순차적 의존성이 없으므로, 모든 토큰에 대한 계산을 병렬로 수행할 수 있어 학습 속도가 훨씬 빠릅니다.\n",
            "2.  **장거리 의존성 포착 능력:** 어텐션 메커니즘을 통해 문장 내의 거리가 먼 단어들 간의 관계도 효율적으로 학습할 수 있습니다.\n",
            "3.  **전이 학습 (Transfer Learning)에 유리:** 대규모 데이터셋으로 사전 학습된 Transformer 모델(예: BERT, GPT)은 다양한 하위 태스크에 파인튜닝(fine-tuning)하여 높은 성능을 달성합니다.\n",
            "4.  **다양한 작업에 범용적 적용:** 기계 번역, 텍스트 요약, 질의응답, 텍스트 생성 등 다양한 NLP 태스크는 물론, 이미지 처리(ViT) 등 비전 분야에서도 뛰어난 성능을 보입니다.\n",
            "\n",
            "---\n",
            "\n",
            "### Transformer 모델의 단점\n",
            "\n",
            "1.  **많은 연산량 및 메모리 사용:** 특히 시퀀스 길이가 길어질수록 어텐션 계산의 복잡도가 시퀀스 길이의 제곱(O(n^2))에 비례하여 증가하므로, 매우 긴 시퀀스에는 비효율적일 수 있습니다.\n",
            "2.  **순서 정보의 명시적 학습 필요:** 위치 임베딩을 통해 순서 정보를 주입해야 합니다.\n",
            "3.  **대규모 데이터셋 요구:** 처음부터 학습시키려면 매우 많은 양의 데이터가 필요합니다.\n",
            "\n",
            "---\n",
            "\n",
            "### 결론\n",
            "\n",
            "Transformer 모델은 어텐션 메커니즘을 전적으로 활용하여 기존 시퀀스 모델의 한계를 극복하고 NLP 분야의 패러다임을 바꾼 혁명적인 아키텍처입니다. 병렬 처리 능력, 장거리 의존성 포착 능력, 그리고 뛰어난 확장성을 바탕으로 현대 인공지능 연구의 핵심적인 기반 기술이 되었습니다.\n",
            "\n",
            "================================================================================\n",
            "RAG 사용\n",
            "================================================================================\n",
            "Transformer는 2010년 영화사에서 만들어낸 인공지능 모델입니다. 이 모델은 어텐션 메커니즘을 핵심으로 하며, BERT, GPT와 같은 최신 언어 모델의 기반이 되었습니다.\n",
            "\n",
            "================================================================================\n",
            "차이점:\n",
            "- RAG를 사용하면 제공된 문서를 근거로 더 정확하고 구체적인 답변을 생성합니다\n",
            "- 일반 LLM은 학습 시점의 지식만 사용하지만, RAG는 최신 문서를 활용할 수 있습니다\n"
          ]
        }
      ],
      "source": [
        "query = \"Transformer 모델에 대해 설명해주세요.\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"일반 LLM (RAG 없이)\")\n",
        "print(\"=\" * 80)\n",
        "response_normal = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=query\n",
        ")\n",
        "print(response_normal.text)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RAG 사용\")\n",
        "print(\"=\" * 80)\n",
        "answer_rag, relevant_chunks = rag_query(query, vector_store, top_k=2)\n",
        "print(answer_rag)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"차이점:\")\n",
        "print(\"- RAG를 사용하면 제공된 문서를 근거로 더 정확하고 구체적인 답변을 생성합니다\")\n",
        "print(\"- 일반 LLM은 학습 시점의 지식만 사용하지만, RAG는 최신 문서를 활용할 수 있습니다\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1a03ac0",
      "metadata": {
        "id": "d1a03ac0"
      },
      "source": [
        "## 10. 실습: RAG 시스템 개선하기\n",
        "\n",
        "위의 코드를 수정하여 다음을 시도해보세요:\n",
        "1. 더 많은 문서를 추가해보기\n",
        "2. 청크 크기와 overlap을 조정해보기\n",
        "3. 검색할 상위 k개 문서 수를 변경해보기\n",
        "4. 프롬프트를 개선하여 더 나은 답변 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15322245",
      "metadata": {
        "id": "15322245"
      },
      "outputs": [],
      "source": [
        "# 실습 공간\n",
        "# 여기서 자유롭게 실험해보세요!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a22b572",
      "metadata": {
        "id": "6a22b572"
      },
      "source": [
        "## 요약\n",
        "\n",
        "RAG 시스템은 다음 단계로 구성됩니다:\n",
        "\n",
        "1. **Source**: 다양한 소스에서 문서 수집 (Text, PPT, Image, PDF, HTML 등)\n",
        "2. **Load**: 다양한 소스에서 문서 로드 (Web Site, DB, YouTube 등)\n",
        "3. **Transform**: 문서를 적절한 크기의 청크로 분할 (데이터 변환 및 정제)\n",
        "4. **Embed**: 각 청크를 임베딩 벡터로 변환 (유사한 텍스트 부분을 빠르고 효율적으로 검색)\n",
        "5. **Store**: 임베딩을 벡터 저장소에 저장 (효율적인 저장 및 검색을 지원하는 데이터베이스)\n",
        "6. **Retrieve**: 질문과 유사한 문서를 검색 (검색 알고리즘을 통한 문서 유사도 측정)\n",
        "7. **Prompt**: 검색된 문서와 질문을 결합하여 프롬프트 생성\n",
        "8. **LLM**: LLM으로 답변 생성\n",
        "9. **Answer**: 최종 답변 반환\n",
        "\n",
        "이 과정을 통해 LLM은 최신 정보와 특정 도메인 지식을 활용하여 더 정확한 답변을 생성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1e679c",
      "metadata": {
        "id": "9d1e679c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}