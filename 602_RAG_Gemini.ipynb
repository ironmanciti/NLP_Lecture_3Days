{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e88b86f",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# 602. RAG (Retrieval Augmented Generation) with Gemini API\n",
    "\n",
    "- Gemini Embedding API를 이용한 문서 임베딩\n",
    "- 유사도 검색을 통한 관련 문서 검색\n",
    "- Gemini API를 이용한 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3830c",
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# Gemini API 클라이언트 초기화\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045912f",
   "metadata": {
    "id": "documents_section"
   },
   "source": [
    "## 1. 문서 데이터 준비\n",
    "\n",
    "실제 사용 시에는 PDF, 웹사이트, 데이터베이스 등에서 문서를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662b8f6",
   "metadata": {
    "id": "documents",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 샘플 문서 데이터 (실제로는 외부 소스에서 로드)\n",
    "documents = [\n",
    "    \"인공지능(AI)은 컴퓨터 시스템이 인간의 지능을 모방하여 학습, 추론, 문제 해결 등의 작업을 수행할 수 있도록 하는 기술입니다. 머신러닝과 딥러닝은 AI의 하위 분야로, 대량의 데이터를 통해 패턴을 학습합니다.\",\n",
    "    \n",
    "    \"자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 AI의 한 분야입니다. 텍스트 분석, 번역, 감성 분석, 챗봇 등 다양한 응용 분야가 있습니다.\",\n",
    "    \n",
    "    \"Transformer는 2017년 Google에서 제안한 딥러닝 아키텍처로, 어텐션 메커니즘을 핵심으로 합니다. BERT, GPT 등 최신 언어 모델의 기반이 되었습니다.\",\n",
    "    \n",
    "    \"RAG(Retrieval Augmented Generation)는 외부 지식 베이스에서 관련 정보를 검색하여 LLM의 답변을 보강하는 기법입니다. 이를 통해 모델의 최신 정보 접근과 정확도가 향상됩니다.\",\n",
    "    \n",
    "    \"벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색할 수 있는 데이터베이스입니다. 임베딩 벡터를 저장하고 유사도 검색에 활용됩니다.\"\n",
    "]\n",
    "\n",
    "print(f\"총 {len(documents)}개의 문서가 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2b367",
   "metadata": {
    "id": "embedding_section"
   },
   "source": [
    "## 2. 문서를 임베딩으로 변환\n",
    "\n",
    "각 문서를 Gemini Embedding API를 사용하여 벡터로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204abe7d",
   "metadata": {
    "id": "embed_function",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def embed_texts(texts, model=\"gemini-embedding-001\", task_type=\"SEMANTIC_SIMILARITY\"):\n",
    "    \"\"\"\n",
    "    텍스트 리스트를 임베딩 벡터로 변환\n",
    "    \n",
    "    Args:\n",
    "        texts: 임베딩할 텍스트 리스트\n",
    "        model: 사용할 임베딩 모델\n",
    "        task_type: 작업 유형 (\"SEMANTIC_SIMILARITY\", \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\" 등)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: 임베딩 벡터 행렬\n",
    "    \"\"\"\n",
    "    result = client.models.embed_content(\n",
    "        model=model,\n",
    "        contents=texts,\n",
    "        config=types.EmbedContentConfig(task_type=task_type)\n",
    "    )\n",
    "    \n",
    "    # 각 임베딩을 numpy 배열로 변환\n",
    "    embeddings = np.array([np.array(e.values) for e in result.embeddings])\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3689227",
   "metadata": {
    "id": "embed_documents",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 문서 임베딩 생성\n",
    "document_embeddings = embed_texts(\n",
    "    documents, \n",
    "    task_type=\"RETRIEVAL_DOCUMENT\"  # 문서 임베딩용\n",
    ")\n",
    "\n",
    "print(f\"문서 임베딩 shape: {document_embeddings.shape}\")\n",
    "print(f\"각 문서는 {document_embeddings.shape[1]}차원 벡터로 표현됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6d4b5",
   "metadata": {
    "id": "search_section"
   },
   "source": [
    "## 3. 쿼리 임베딩 및 유사도 검색\n",
    "\n",
    "사용자 쿼리를 임베딩으로 변환하고, 문서 임베딩과의 유사도를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846f76d",
   "metadata": {
    "id": "search_function",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def search_relevant_documents(query, document_embeddings, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    쿼리와 가장 유사한 문서를 검색\n",
    "    \n",
    "    Args:\n",
    "        query: 사용자 쿼리\n",
    "        document_embeddings: 문서 임베딩 행렬\n",
    "        documents: 원본 문서 리스트\n",
    "        top_k: 반환할 상위 문서 개수\n",
    "    \n",
    "    Returns:\n",
    "        list: (유사도, 문서) 튜플 리스트\n",
    "    \"\"\"\n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding = embed_texts(\n",
    "        [query],\n",
    "        task_type=\"RETRIEVAL_QUERY\"  # 쿼리 임베딩용\n",
    "    )[0]\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "    \n",
    "    # 상위 k개 문서 선택\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # 결과 반환\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append((similarities[idx], documents[idx]))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3b019",
   "metadata": {
    "id": "rag_section"
   },
   "source": [
    "## 4. RAG 시스템 구현\n",
    "\n",
    "검색된 문서를 컨텍스트로 사용하여 Gemini API로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5960f90",
   "metadata": {
    "id": "rag_function",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def rag_query(query, documents, document_embeddings, top_k=3):\n",
    "    \"\"\"\n",
    "    RAG를 사용하여 쿼리에 대한 답변 생성\n",
    "    \n",
    "    Args:\n",
    "        query: 사용자 쿼리\n",
    "        documents: 문서 리스트\n",
    "        document_embeddings: 문서 임베딩 행렬\n",
    "        top_k: 검색할 상위 문서 개수\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (생성된 답변, 관련 문서 리스트)\n",
    "    \"\"\"\n",
    "    # 1. 관련 문서 검색\n",
    "    relevant_docs = search_relevant_documents(\n",
    "        query, document_embeddings, documents, top_k\n",
    "    )\n",
    "    \n",
    "    # 2. 컨텍스트 구성\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[문서 {i+1}] {doc}\" \n",
    "        for i, (score, doc) in enumerate(relevant_docs)\n",
    "    ])\n",
    "    \n",
    "    # 3. 프롬프트 구성\n",
    "    prompt = f\"\"\"다음 문서들을 참고하여 질문에 답변해주세요.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {query}\n",
    "\n",
    "답변:\"\"\"\n",
    "    \n",
    "    # 4. Gemini API로 답변 생성\n",
    "    model = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    return model.text, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955d395",
   "metadata": {
    "id": "test_section"
   },
   "source": [
    "## 5. RAG 시스템 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce37b8",
   "metadata": {
    "id": "test_queries"
   },
   "outputs": [],
   "source": [
    "# 테스트 쿼리들\n",
    "test_queries = [\n",
    "    \"인공지능이란 무엇인가요?\",\n",
    "    \"RAG는 어떻게 작동하나요?\",\n",
    "    \"Transformer 모델에 대해 설명해주세요.\",\n",
    "    \"벡터 데이터베이스는 무엇인가요?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258927cd",
   "metadata": {
    "id": "run_rag",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "for query in test_queries:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"질문: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # RAG로 답변 생성\n",
    "    answer, relevant_docs = rag_query(query, documents, document_embeddings, top_k=2)\n",
    "    \n",
    "    # 검색된 문서 출력\n",
    "    print(\"\\n[참고 문서]\")\n",
    "    for i, (score, doc) in enumerate(relevant_docs, 1):\n",
    "        print(f\"{i}. (유사도: {score:.4f}) {doc[:100]}...\")\n",
    "    \n",
    "    # 생성된 답변 출력\n",
    "    print(f\"\\n[답변]\")\n",
    "    print(answer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009fa8f",
   "metadata": {
    "id": "visualization_section"
   },
   "source": [
    "## 6. 유사도 검색 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4298846",
   "metadata": {
    "id": "similarity_visualization",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_similarities(query, documents, document_embeddings):\n",
    "    \"\"\"\n",
    "    쿼리와 문서들 간의 유사도를 시각화\n",
    "    \"\"\"\n",
    "    # 쿼리 임베딩\n",
    "    query_embedding = embed_texts(\n",
    "        [query],\n",
    "        task_type=\"RETRIEVAL_QUERY\"\n",
    "    )[0]\n",
    "    \n",
    "    # 유사도 계산\n",
    "    similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(documents)), similarities)\n",
    "    plt.yticks(range(len(documents)), [f\"문서 {i+1}\" for i in range(len(documents))])\n",
    "    plt.xlabel(\"코사인 유사도\")\n",
    "    plt.title(f\"쿼리: '{query}'\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f148f6e",
   "metadata": {
    "id": "visualize_example",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 예시 시각화\n",
    "similarities = visualize_similarities(\n",
    "    \"인공지능과 머신러닝의 차이는?\",\n",
    "    documents,\n",
    "    document_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a03ac0",
   "metadata": {
    "id": "chunking_section"
   },
   "source": [
    "## 7. 실제 문서 로드 예제 (선택사항)\n",
    "\n",
    "실제 파일에서 문서를 로드하는 예제입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15322245",
   "metadata": {
    "id": "load_documents",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_documents_from_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    긴 텍스트를 청크로 분할\n",
    "    \n",
    "    Args:\n",
    "        text: 분할할 텍스트\n",
    "        chunk_size: 각 청크의 크기 (문자 수)\n",
    "        overlap: 청크 간 겹치는 부분 (문자 수)\n",
    "    \n",
    "    Returns:\n",
    "        list: 청크 리스트\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b518dcc",
   "metadata": {
    "id": "chunking_example",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 예시: 긴 텍스트를 청크로 분할\n",
    "long_text = \"\"\"\n",
    "인공지능(AI)은 컴퓨터 시스템이 인간의 지능을 모방하여 학습, 추론, 문제 해결 등의 작업을 수행할 수 있도록 하는 기술입니다.\n",
    "머신러닝은 AI의 하위 분야로, 명시적인 프로그래밍 없이 데이터로부터 학습하는 능력을 컴퓨터에 부여합니다.\n",
    "딥러닝은 머신러닝의 한 분야로, 인공 신경망을 여러 층으로 쌓아 복잡한 패턴을 학습합니다.\n",
    "자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 AI의 한 분야입니다.\n",
    "\"\"\"\n",
    "\n",
    "chunks = load_documents_from_text(long_text, chunk_size=100, overlap=20)\n",
    "print(f\"텍스트가 {len(chunks)}개의 청크로 분할되었습니다.\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\n청크 {i}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90301d7",
   "metadata": {
    "id": "enhanced_section"
   },
   "source": [
    "## 8. 개선된 RAG 시스템 (메타데이터 포함)\n",
    "\n",
    "문서에 메타데이터를 추가하여 더 정확한 검색이 가능하도록 개선합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4ddbe",
   "metadata": {
    "id": "document_class",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"문서와 메타데이터를 포함하는 클래스\"\"\"\n",
    "    def __init__(self, content, metadata=None):\n",
    "        self.content = content\n",
    "        self.metadata = metadata or {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c3f37",
   "metadata": {
    "id": "create_documents"
   },
   "outputs": [],
   "source": [
    "# 문서 객체 생성\n",
    "document_objects = [\n",
    "    Document(\n",
    "        documents[0],\n",
    "        {\"category\": \"AI 기본\", \"source\": \"교재 1장\"}\n",
    "    ),\n",
    "    Document(\n",
    "        documents[1],\n",
    "        {\"category\": \"NLP\", \"source\": \"교재 2장\"}\n",
    "    ),\n",
    "    Document(\n",
    "        documents[2],\n",
    "        {\"category\": \"딥러닝\", \"source\": \"교재 3장\"}\n",
    "    ),\n",
    "    Document(\n",
    "        documents[3],\n",
    "        {\"category\": \"RAG\", \"source\": \"교재 4장\"}\n",
    "    ),\n",
    "    Document(\n",
    "        documents[4],\n",
    "        {\"category\": \"데이터베이스\", \"source\": \"교재 5장\"}\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d6333",
   "metadata": {
    "id": "enhanced_embeddings",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 문서 내용만 추출하여 임베딩\n",
    "document_contents = [str(doc) for doc in document_objects]\n",
    "document_embeddings_enhanced = embed_texts(\n",
    "    document_contents,\n",
    "    task_type=\"RETRIEVAL_DOCUMENT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75b03b",
   "metadata": {
    "id": "enhanced_rag_function",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def enhanced_rag_query(query, document_objects, document_embeddings, top_k=3):\n",
    "    \"\"\"\n",
    "    메타데이터를 포함한 개선된 RAG 쿼리\n",
    "    \"\"\"\n",
    "    # 검색\n",
    "    document_contents = [str(doc) for doc in document_objects]\n",
    "    relevant_docs = search_relevant_documents(\n",
    "        query, document_embeddings, document_contents, top_k\n",
    "    )\n",
    "    \n",
    "    # 메타데이터와 함께 컨텍스트 구성\n",
    "    context_parts = []\n",
    "    for i, (score, content) in enumerate(relevant_docs):\n",
    "        # 해당 문서의 메타데이터 찾기\n",
    "        doc_obj = next(\n",
    "            (d for d in document_objects if d.content == content),\n",
    "            None\n",
    "        )\n",
    "        metadata_str = \"\"\n",
    "        if doc_obj and doc_obj.metadata:\n",
    "            metadata_str = f\" (카테고리: {doc_obj.metadata.get('category', 'N/A')}, 출처: {doc_obj.metadata.get('source', 'N/A')})\"\n",
    "        \n",
    "        context_parts.append(f\"[문서 {i+1}]{metadata_str}\\n{content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 프롬프트 구성\n",
    "    prompt = f\"\"\"다음 문서들을 참고하여 질문에 답변해주세요.\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {query}\n",
    "\n",
    "답변:\"\"\"\n",
    "    \n",
    "    # 답변 생성\n",
    "    model = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    return model.text, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445dd2da",
   "metadata": {
    "id": "test_enhanced_rag"
   },
   "outputs": [],
   "source": [
    "# 개선된 RAG 테스트\n",
    "print(\"=\" * 80)\n",
    "print(\"개선된 RAG 시스템 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "answer, relevant_docs = enhanced_rag_query(\n",
    "    \"RAG 시스템은 어떻게 작동하나요?\",\n",
    "    document_objects,\n",
    "    document_embeddings_enhanced,\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "print(f\"\\n[답변]\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e679c",
   "metadata": {
    "id": "end",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
