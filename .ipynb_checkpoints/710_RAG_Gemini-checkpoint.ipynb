{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e88b86f",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# RAG (Retrieval Augmented Generation) 실습\n",
    "\n",
    "RAG는 LLM이 내부 지식만으로 답하는 방식이 아니라, 질문과 관련된 문서를 추가로 제공받아 그것을 근거로 답변을 생성하는 방법입니다.\n",
    "\n",
    "## RAG Flow\n",
    "1. **Document Ingestion**: 문서를 작은 청크(chunk)로 분할하고 임베딩으로 변환하여 벡터 저장소에 저장\n",
    "2. **Query Processing**: 사용자 질문을 임베딩으로 변환\n",
    "3. **Retrieval**: 질문 임베딩과 유사한 문서 청크를 검색\n",
    "4. **Generation**: 검색된 문서와 질문을 함께 LLM에 제공하여 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3830c",
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# Gemini API 클라이언트 초기화\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045912f",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## 1. Source: 문서 데이터 준비\n",
    "\n",
    "RAG 시스템의 첫 단계는 다양한 소스에서 문서를 준비하는 것입니다.\n",
    "실제 환경에서는 PDF, 웹사이트, 데이터베이스 등에서 문서를 로드합니다.\n",
    "Text, PPT, Image, PDF, HTML 등 다양한 비정형 데이터 소스를 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662b8f6",
   "metadata": {
    "id": "documents",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 샘플 문서 데이터 (실제로는 외부 소스에서 로드)\n",
    "documents = [\n",
    "    \"인공지능(AI)은 컴퓨터 시스템이 인간의 지능을 모방하여 학습, 추론, 문제 해결 등의 작업을 수행할 수 있도록 하는 기술입니다. 머신러닝과 딥러닝은 AI의 하위 분야로, 대량의 데이터를 통해 패턴을 학습합니다.\",\n",
    "    \n",
    "    \"자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 AI의 한 분야입니다. 텍스트 분석, 번역, 감성 분석, 챗봇 등 다양한 응용 분야가 있습니다.\",\n",
    "    \n",
    "    \"Transformer는 2017년 Google에서 제안한 딥러닝 아키텍처로, 어텐션 메커니즘을 핵심으로 합니다. BERT, GPT 등 최신 언어 모델의 기반이 되었습니다.\",\n",
    "    \n",
    "    \"RAG(Retrieval Augmented Generation)는 외부 지식 베이스에서 관련 정보를 검색하여 LLM의 답변을 보강하는 기법입니다. 이를 통해 모델의 최신 정보 접근과 정확도가 향상됩니다.\",\n",
    "    \n",
    "    \"벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색할 수 있는 데이터베이스입니다. 임베딩 벡터를 저장하고 유사도 검색에 활용됩니다.\"\n",
    "]\n",
    "\n",
    "print(f\"총 {len(documents)}개의 문서가 준비되었습니다.\\n\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"문서 {i}: {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2b367",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## 2. Load & Transform: 문서를 청크로 분할\n",
    "\n",
    "다양한 소스(Web Site, DB, YouTube 등)에서 문서(HTML, PDF, JSON, Word, PPT, 코드 등)를 로드합니다.\n",
    "긴 문서는 작은 단위(청크)로 나누어야 합니다. 이렇게 하면:\n",
    "- 임베딩 생성이 효율적입니다\n",
    "- 검색 시 더 정확한 관련 부분을 찾을 수 있습니다\n",
    "- LLM의 컨텍스트 길이 제한을 고려할 수 있습니다\n",
    "\n",
    "데이터 변환 및 정제를 위한 여러 변환 단계를 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204abe7d",
   "metadata": {
    "id": "chunking",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def split_into_chunks(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    긴 텍스트를 청크로 분할\n",
    "    \n",
    "    Args:\n",
    "        text: 분할할 텍스트\n",
    "        chunk_size: 각 청크의 크기 (문자 수)\n",
    "        overlap: 청크 간 겹치는 부분 (문자 수)\n",
    "    \n",
    "    Returns:\n",
    "        list: 청크 리스트\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# 모든 문서를 청크로 분할\n",
    "all_chunks = []\n",
    "for i, doc in enumerate(documents):\n",
    "    chunks = split_into_chunks(doc, chunk_size=150, overlap=30)\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"문서 {i+1}이 {len(chunks)}개의 청크로 분할되었습니다.\")\n",
    "\n",
    "print(f\"\\n총 {len(all_chunks)}개의 청크가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6e963",
   "metadata": {
    "id": "section3"
   },
   "source": [
    "## 3. Embed: 문서를 임베딩으로 변환\n",
    "\n",
    "각 문서 청크를 숫자 벡터(임베딩)로 변환합니다.\n",
    "유사한 의미를 가진 텍스트는 유사한 벡터로 표현됩니다.\n",
    "문서에 대한 임베딩을 만들어 유사한 다른 텍스트 부분을 빠르고 효율적으로 검색할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3689227",
   "metadata": {
    "id": "embedding_function"
   },
   "outputs": [],
   "source": [
    "def embed_texts(texts, model=\"gemini-embedding-001\", task_type=\"RETRIEVAL_DOCUMENT\"):\n",
    "    \"\"\"\n",
    "    텍스트 리스트를 임베딩 벡터로 변환\n",
    "    \n",
    "    Args:\n",
    "        texts: 임베딩할 텍스트 리스트\n",
    "        model: 사용할 임베딩 모델\n",
    "        task_type: 작업 유형 (\"SEMANTIC_SIMILARITY\", \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\" 등)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: 임베딩 벡터 행렬\n",
    "    \"\"\"\n",
    "    result = client.models.embed_content(\n",
    "        model=model,\n",
    "        contents=texts,\n",
    "        config=types.EmbedContentConfig(task_type=task_type)\n",
    "    )\n",
    "    \n",
    "    # 각 임베딩을 numpy 배열로 변환\n",
    "    embeddings = np.array([np.array(e.values) for e in result.embeddings])\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# 모든 청크를 임베딩으로 변환\n",
    "print(\"청크를 임베딩으로 변환 중...\")\n",
    "chunk_embeddings = embed_texts(all_chunks, task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "\n",
    "print(f\"\\n임베딩 완료! Shape: {chunk_embeddings.shape}\")\n",
    "print(f\"각 청크는 {chunk_embeddings.shape[1]}차원 벡터로 표현됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a6d4b5",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## 4. Store: 임베딩을 벡터 저장소에 저장\n",
    "\n",
    "생성된 임베딩을 벡터 저장소에 저장합니다.\n",
    "실제 환경에서는 Pinecone, Weaviate, ChromaDB 등의 벡터 데이터베이스를 사용합니다.\n",
    "임베딩의 효율적인 저장 및 검색을 지원하는 데이터베이스입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846f76d",
   "metadata": {
    "id": "store",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 벡터 저장소 (실제로는 벡터 DB를 사용)\n",
    "vector_store = {\n",
    "    'chunks': all_chunks,\n",
    "    'embeddings': chunk_embeddings\n",
    "}\n",
    "\n",
    "print(\"벡터 저장소에 저장 완료!\")\n",
    "print(f\"- 저장된 청크 수: {len(vector_store['chunks'])}\")\n",
    "print(f\"- 임베딩 벡터 shape: {vector_store['embeddings'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3b019",
   "metadata": {
    "id": "section5"
   },
   "source": [
    "## 5. Retrieve: 질문과 유사한 문서 검색\n",
    "\n",
    "사용자 질문을 임베딩으로 변환하고, 저장된 문서 임베딩과의 유사도를 계산하여\n",
    "가장 관련성 높은 문서를 검색합니다.\n",
    "검색 알고리즘을 통해 문서 유사도를 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5960f90",
   "metadata": {
    "id": "retrieve_function",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def search_relevant_chunks(query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    질문과 가장 유사한 청크를 검색\n",
    "    \n",
    "    Args:\n",
    "        query: 사용자 질문\n",
    "        vector_store: 벡터 저장소 (chunks, embeddings 포함)\n",
    "        top_k: 반환할 상위 청크 개수\n",
    "    \n",
    "    Returns:\n",
    "        list: (유사도, 청크) 튜플 리스트\n",
    "    \"\"\"\n",
    "    # 1. 질문을 임베딩으로 변환\n",
    "    query_embedding = embed_texts(\n",
    "        [query],\n",
    "        task_type=\"RETRIEVAL_QUERY\"\n",
    "    )[0]\n",
    "    \n",
    "    # 2. 모든 청크와의 유사도 계산\n",
    "    similarities = cosine_similarity([query_embedding], vector_store['embeddings'])[0]\n",
    "    \n",
    "    # 3. 상위 k개 선택\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # 4. 결과 반환\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append((similarities[idx], vector_store['chunks'][idx]))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 검색 테스트\n",
    "test_query = \"인공지능이란 무엇인가요?\"\n",
    "print(f\"질문: {test_query}\\n\")\n",
    "print(\"검색 결과:\")\n",
    "relevant_chunks = search_relevant_chunks(test_query, vector_store, top_k=3)\n",
    "for i, (score, chunk) in enumerate(relevant_chunks, 1):\n",
    "    print(f\"{i}. (유사도: {score:.4f}) {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955d395",
   "metadata": {
    "id": "section6"
   },
   "source": [
    "## 6. RAG 시스템 구현\n",
    "\n",
    "검색된 문서를 컨텍스트로 사용하여 Gemini API로 답변을 생성합니다.\n",
    "이것이 RAG의 핵심입니다: 검색(Retrieval) + 생성(Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce37b8",
   "metadata": {
    "id": "rag_function",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def rag_query(query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    RAG를 사용하여 쿼리에 대한 답변 생성\n",
    "    \n",
    "    Args:\n",
    "        query: 사용자 질문\n",
    "        vector_store: 벡터 저장소\n",
    "        top_k: 검색할 상위 청크 개수\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (생성된 답변, 관련 청크 리스트)\n",
    "    \"\"\"\n",
    "    # 1. 관련 문서 검색\n",
    "    relevant_chunks = search_relevant_chunks(query, vector_store, top_k)\n",
    "    \n",
    "    # 2. 컨텍스트 구성\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[참고 문서 {i+1}] {chunk}\" \n",
    "        for i, (score, chunk) in enumerate(relevant_chunks)\n",
    "    ])\n",
    "    \n",
    "    # 3. 프롬프트 구성\n",
    "    prompt = f\"\"\"다음 문서들을 참고하여 질문에 답변해주세요.\n",
    "    \n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {query}\n",
    "\n",
    "답변:\"\"\"\n",
    "    \n",
    "    # 4. Gemini API로 답변 생성\n",
    "    model = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    return model.text, relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbb952",
   "metadata": {
    "id": "section7"
   },
   "source": [
    "## 7. RAG 시스템 테스트\n",
    "\n",
    "다양한 질문으로 RAG 시스템을 테스트해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258927cd",
   "metadata": {
    "id": "test_rag"
   },
   "outputs": [],
   "source": [
    "# 테스트 쿼리들\n",
    "test_queries = [\n",
    "    \"인공지능이란 무엇인가요?\",\n",
    "    \"RAG는 어떻게 작동하나요?\",\n",
    "    \"Transformer 모델에 대해 설명해주세요.\",\n",
    "    \"벡터 데이터베이스는 무엇인가요?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"질문: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # RAG로 답변 생성\n",
    "    answer, relevant_chunks = rag_query(query, vector_store, top_k=2)\n",
    "    \n",
    "    # 검색된 문서 출력\n",
    "    print(\"\\n[참고 문서]\")\n",
    "    for i, (score, chunk) in enumerate(relevant_chunks, 1):\n",
    "        print(f\"{i}. (유사도: {score:.4f}) {chunk[:100]}...\")\n",
    "    \n",
    "    # 생성된 답변 출력\n",
    "    print(f\"\\n[답변]\")\n",
    "    print(answer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009fa8f",
   "metadata": {
    "id": "section8"
   },
   "source": [
    "## 8. RAG vs 일반 LLM 비교\n",
    "\n",
    "RAG를 사용하면 LLM이 최신 정보나 특정 도메인 지식을 활용할 수 있습니다.\n",
    "일반 LLM과 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4298846",
   "metadata": {
    "id": "compare",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "query = \"RAG는 어떻게 작동하나요?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"일반 LLM (RAG 없이)\")\n",
    "print(\"=\" * 80)\n",
    "response_normal = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=query\n",
    ")\n",
    "print(response_normal.text)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RAG 사용\")\n",
    "print(\"=\" * 80)\n",
    "answer_rag, relevant_chunks = rag_query(query, vector_store, top_k=2)\n",
    "print(answer_rag)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"차이점:\")\n",
    "print(\"- RAG를 사용하면 제공된 문서를 근거로 더 정확하고 구체적인 답변을 생성합니다\")\n",
    "print(\"- 일반 LLM은 학습 시점의 지식만 사용하지만, RAG는 최신 문서를 활용할 수 있습니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1674b5",
   "metadata": {
    "id": "section9"
   },
   "source": [
    "## 9. 유사도 검색 시각화\n",
    "\n",
    "쿼리와 청크들 간의 유사도를 시각화하여 이해를 돕습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f148f6e",
   "metadata": {
    "id": "visualization",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_similarities(query, vector_store):\n",
    "    \"\"\"\n",
    "    쿼리와 청크들 간의 유사도를 시각화\n",
    "    \"\"\"\n",
    "    # 쿼리 임베딩\n",
    "    query_embedding = embed_texts(\n",
    "        [query],\n",
    "        task_type=\"RETRIEVAL_QUERY\"\n",
    "    )[0]\n",
    "    \n",
    "    # 유사도 계산\n",
    "    similarities = cosine_similarity([query_embedding], vector_store['embeddings'])[0]\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(vector_store['chunks'])), similarities)\n",
    "    plt.yticks(range(len(vector_store['chunks'])), \n",
    "                [f\"청크 {i+1}\" for i in range(len(vector_store['chunks']))])\n",
    "    plt.xlabel(\"코사인 유사도\")\n",
    "    plt.title(f\"쿼리: '{query}'\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05cd00",
   "metadata": {
    "id": "visualize_example",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 예시 시각화\n",
    "similarities = visualize_similarities(\n",
    "    \"인공지능과 머신러닝의 차이는?\",\n",
    "    vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a03ac0",
   "metadata": {
    "id": "section10"
   },
   "source": [
    "## 10. 실습: RAG 시스템 개선하기\n",
    "\n",
    "위의 코드를 수정하여 다음을 시도해보세요:\n",
    "1. 더 많은 문서를 추가해보기\n",
    "2. 청크 크기와 overlap을 조정해보기\n",
    "3. 검색할 상위 k개 문서 수를 변경해보기\n",
    "4. 프롬프트를 개선하여 더 나은 답변 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15322245",
   "metadata": {
    "id": "practice"
   },
   "outputs": [],
   "source": [
    "# 실습 공간\n",
    "# 여기서 자유롭게 실험해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943ab55",
   "metadata": {
    "id": "enhanced_section"
   },
   "source": [
    "## 11. 개선된 RAG 시스템 (메타데이터 포함) - 선택사항\n",
    "\n",
    "문서에 메타데이터를 추가하여 더 정확한 검색이 가능하도록 개선합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4ddbe",
   "metadata": {
    "id": "document_class",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"문서와 메타데이터를 포함하는 클래스\"\"\"\n",
    "    def __init__(self, content, metadata=None):\n",
    "        self.content = content\n",
    "        self.metadata = metadata or {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c3f37",
   "metadata": {
    "id": "create_documents"
   },
   "outputs": [],
   "source": [
    "# 문서 객체 생성\n",
    "document_objects = [\n",
    "    Document(\n",
    "        documents[0],\n",
    "        {\"category\": \"AI 기본\", \"source\": \"교재 1장\"}\n",
    "    ),\n",
    "    Document(\n",
    "        documents[1],\n",
    "        {\"category\": \"NLP\", \"source\": \"교재 2장\"}\n",
    "    ),\n",
    "    Document(\n",
    "        documents[2],\n",
    "        {\"category\": \"딥러닝\", \"source\": \"교재 3장\"}\n",
    "    ),\n",
    "    Document(\n",
    "        documents[3],\n",
    "        {\"category\": \"RAG\", \"source\": \"교재 4장\"}\n",
    "    ),\n",
    "    Document(\n",
    "        documents[4],\n",
    "        {\"category\": \"데이터베이스\", \"source\": \"교재 5장\"}\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d6333",
   "metadata": {
    "id": "enhanced_embeddings",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 문서 내용만 추출하여 임베딩\n",
    "document_contents = [str(doc) for doc in document_objects]\n",
    "document_embeddings_enhanced = embed_texts(\n",
    "    document_contents,\n",
    "    task_type=\"RETRIEVAL_DOCUMENT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75b03b",
   "metadata": {
    "id": "enhanced_rag_function",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def enhanced_rag_query(query, document_objects, document_embeddings, top_k=3):\n",
    "    \"\"\"\n",
    "    메타데이터를 포함한 개선된 RAG 쿼리\n",
    "    \"\"\"\n",
    "    # 검색\n",
    "    document_contents = [str(doc) for doc in document_objects]\n",
    "    relevant_docs = search_relevant_documents(\n",
    "        query, document_embeddings, document_contents, top_k\n",
    "    )\n",
    "    \n",
    "    # 메타데이터와 함께 컨텍스트 구성\n",
    "    context_parts = []\n",
    "    for i, (score, content) in enumerate(relevant_docs):\n",
    "        # 해당 문서의 메타데이터 찾기\n",
    "        doc_obj = next(\n",
    "            (d for d in document_objects if d.content == content),\n",
    "            None\n",
    "        )\n",
    "        metadata_str = \"\"\n",
    "        if doc_obj and doc_obj.metadata:\n",
    "            metadata_str = f\" (카테고리: {doc_obj.metadata.get('category', 'N/A')}, 출처: {doc_obj.metadata.get('source', 'N/A')})\"\n",
    "        \n",
    "        context_parts.append(f\"[문서 {i+1}]{metadata_str}\\n{content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 프롬프트 구성\n",
    "    prompt = f\"\"\"다음 문서들을 참고하여 질문에 답변해주세요.\n",
    "    \n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {query}\n",
    "\n",
    "답변:\"\"\"\n",
    "    \n",
    "    # 답변 생성\n",
    "    model = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    return model.text, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20597f29",
   "metadata": {
    "id": "search_relevant_documents",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def search_relevant_documents(query, document_embeddings, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    쿼리와 가장 유사한 문서를 검색\n",
    "    \n",
    "    Args:\n",
    "        query: 사용자 쿼리\n",
    "        document_embeddings: 문서 임베딩 행렬\n",
    "        documents: 원본 문서 리스트\n",
    "        top_k: 반환할 상위 문서 개수\n",
    "    \n",
    "    Returns:\n",
    "        list: (유사도, 문서) 튜플 리스트\n",
    "    \"\"\"\n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding = embed_texts(\n",
    "        [query],\n",
    "        task_type=\"RETRIEVAL_QUERY\"\n",
    "    )[0]\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "    \n",
    "    # 상위 k개 문서 선택\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # 결과 반환\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append((similarities[idx], documents[idx]))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445dd2da",
   "metadata": {
    "id": "test_enhanced_rag"
   },
   "outputs": [],
   "source": [
    "# 개선된 RAG 테스트\n",
    "print(\"=\" * 80)\n",
    "print(\"개선된 RAG 시스템 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "answer, relevant_docs = enhanced_rag_query(\n",
    "    \"RAG 시스템은 어떻게 작동하나요?\",\n",
    "    document_objects,\n",
    "    document_embeddings_enhanced,\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "print(f\"\\n[답변]\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22b572",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 요약\n",
    "\n",
    "RAG 시스템은 다음 단계로 구성됩니다:\n",
    "\n",
    "1. **Source**: 다양한 소스에서 문서 수집 (Text, PPT, Image, PDF, HTML 등)\n",
    "2. **Load**: 다양한 소스에서 문서 로드 (Web Site, DB, YouTube 등)\n",
    "3. **Transform**: 문서를 적절한 크기의 청크로 분할 (데이터 변환 및 정제)\n",
    "4. **Embed**: 각 청크를 임베딩 벡터로 변환 (유사한 텍스트 부분을 빠르고 효율적으로 검색)\n",
    "5. **Store**: 임베딩을 벡터 저장소에 저장 (효율적인 저장 및 검색을 지원하는 데이터베이스)\n",
    "6. **Retrieve**: 질문과 유사한 문서를 검색 (검색 알고리즘을 통한 문서 유사도 측정)\n",
    "7. **Prompt**: 검색된 문서와 질문을 결합하여 프롬프트 생성\n",
    "8. **LLM**: LLM으로 답변 생성\n",
    "9. **Answer**: 최종 답변 반환\n",
    "\n",
    "이 과정을 통해 LLM은 최신 정보와 특정 도메인 지식을 활용하여 더 정확한 답변을 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e679c",
   "metadata": {
    "id": "end"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
