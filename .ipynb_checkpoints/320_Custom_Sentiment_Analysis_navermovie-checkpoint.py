# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.17.3
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown] id="C2F7l-nFg1A2"
# # 320. Custom Datasetì„ ì´ìš©í•œ Hugging Face BERT model Fine Tuning
#
# - NAVER Movie review datasetì„ ì´ìš©í•˜ì—¬ transformers BERT modelì„ fine tuning  â†’ ê°ì„±ë¶„ì„ ëª¨ë¸ ì‘ì„±
#
# - Pytorch ì™€ Trainerë¥¼ ì´ìš©í•œ Fine Tuning (Pytorch versionì´ Tensorflow ë³´ë‹¤ ì•ˆì •ì )
#
# - Colab gpu ì‚¬ìš©

# %% id="xrmKb41C0RSy"
# Hugging Face Transformersì—ì„œ BERT í† í¬ë‚˜ì´ì € ë¡œë“œ
from transformers import BertTokenizer
# Hugging Face Transformersì—ì„œ BERT ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
import torch.nn.functional as F
import tensorflow as tf
import torch
import pandas as pd

# %% id="KH9fBvHtNoQi"
DATA_TRAIN_PATH = tf.keras.utils.get_file("ratings_train.txt",
                     "https://raw.github.com/ironmanciti/Infran_NLP/master/data/naver_movie/ratings_train.txt")
DATA_TEST_PATH = tf.keras.utils.get_file("ratings_test.txt",
                    "https://raw.github.com/ironmanciti/Infran_NLP/master/data/naver_movie/ratings_test.txt")

# %% [markdown] id="0mmyt3b5Ul95"
# ### Train Set

# %% colab={"base_uri": "https://localhost:8080/", "height": 224} id="D30MwdfZNq8C" outputId="a34aced0-bef7-4a6b-999e-3a3aefeeb27c"
#  í•™ìŠµ ë°ì´í„° ë¡œë“œ
train_data = pd.read_csv(DATA_TRAIN_PATH, delimiter='\t')

print(train_data.shape)
train_data.head()

# %% colab={"base_uri": "https://localhost:8080/"} id="bpfAwxRNOSpd" outputId="92df2544-7090-4bcf-a33f-88dcf835fe59"
# ê²°ì¸¡ê°’(NaN)ì´ í¬í•¨ëœ í–‰ì„ ëª¨ë‘ ì œê±°
train_data.dropna(inplace=True)

# í˜„ì¬ DataFrameì˜ êµ¬ì¡° ìš”ì•½ ì¶œë ¥
train_data.info()

# %% [markdown] id="Wu03MkzTUqem"
# ### Test Set

# %% colab={"base_uri": "https://localhost:8080/", "height": 224} id="Yr-jAjjYOWXo" outputId="99578b5d-58e4-42df-dec6-f5135c24df36"
#  ê²€ì¦ ë°ì´í„° ë¡œë“œ
test_data = pd.read_csv(DATA_TEST_PATH, delimiter='\t')

print(test_data.shape)
test_data.head()

# %% colab={"base_uri": "https://localhost:8080/"} id="t8n29Z8JOZJz" outputId="4c36ec67-3718-4e7f-dd22-11d7194d5148"
# ê²°ì¸¡ê°’(NaN)ì´ í¬í•¨ëœ í–‰ì„ ëª¨ë‘ ì œê±°
test_data.dropna(inplace=True)
test_data.info()

# %% [markdown] id="R-Z6KQk0Uxcd"
# - í›ˆë ¨ ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ data size ì¶•ì†Œ

# %% colab={"base_uri": "https://localhost:8080/"} id="EsvEwG98qq2b" outputId="b162d34d-33aa-48da-c894-8b3d54a197fe"
# í›ˆë ¨ ë°ì´í„°ì—ì„œ ë¬´ì‘ìœ„ë¡œ 100,000ê°œ ìƒ˜í”Œ ì¶”ì¶œ (ì¬í˜„ì„±ì„ ìœ„í•´ random_state ê³ ì •)
df_train = train_data.sample(n=15_000, random_state=1)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë¬´ì‘ìœ„ë¡œ 10,000ê°œ ìƒ˜í”Œ ì¶”ì¶œ
df_test = test_data.sample(n=5_000, random_state=1)

# ì¶”ì¶œëœ ë°ì´í„°í”„ë ˆì„ì˜ í–‰ê³¼ ì—´ í¬ê¸° ì¶œë ¥
print(df_train.shape)
print(df_test.shape)

# %% colab={"base_uri": "https://localhost:8080/", "height": 178} id="6bJRCocDrFvE" outputId="566837c4-1e91-40d4-df2a-1fc09dedb7d2"
# í›ˆë ¨ ë°ì´í„°ì˜ 'label' ì—´ì— ìˆëŠ” ê° í´ë˜ìŠ¤(ë ˆì´ë¸”)ë³„ ê°œìˆ˜ë¥¼ ì§‘ê³„
df_train['label'].value_counts()

# %% id="OmDH9gwtP4Oj"
# í›ˆë ¨ ë°ì´í„°ì—ì„œ ì…ë ¥ ë¬¸ì¥(document)ê³¼ ë ˆì´ë¸”(label)ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
X_train = df_train['document'].values.tolist()      # ì…ë ¥ í…ìŠ¤íŠ¸ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
y_train = df_train['label'].values.tolist()               # ì •ë‹µ ë ˆì´ë¸” (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ ë™ì¼í•˜ê²Œ ì…ë ¥ê³¼ ë ˆì´ë¸”ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
X_test = df_test['document'].values.tolist()    # ì…ë ¥ í…ìŠ¤íŠ¸ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
y_test = df_test['label'].values.tolist()             # ì •ë‹µ ë ˆì´ë¸” (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)

# %% [markdown] id="tRmyBvjXfr17"
# ## pre-trained bert model í˜¸ì¶œ
# ### tokenizer í˜¸ì¶œ
# - í† í°í™” ì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤. bert ë‹¤êµ­ì–´ version ìš©ì˜ pre-trained tokenizer ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

# %% colab={"base_uri": "https://localhost:8080/"} id="bcNEJ6perOSs" outputId="f861a7bf-80bf-4d7e-e8fe-a5cec51a2648"
# ì‚¬ì „í•™ìŠµëœ BERT í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
# 'bert-base-multilingual-cased'ëŠ” 100ê°œ ì´ìƒì˜ ì–¸ì–´ë¥¼ ì§€ì›í•˜ëŠ” ë‹¤êµ­ì–´ BERT ëª¨ë¸ë¡œ,
# ëŒ€ì†Œë¬¸ì êµ¬ë¶„(cased)ì„ ìœ ì§€í•¨
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

# %% [markdown] id="QL9aFGw1VZxj"
# pre-trained tokenizer ë¥¼ ì´ìš©í•˜ì—¬ train set ê³¼ test set ì„ token í™” í•©ë‹ˆë‹¤.
#
# - Input IDs : í† í° ì¸ë±ìŠ¤, ëª¨ë¸ì—ì„œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ì‹œí€€ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” í† í°ì˜ ìˆ«ì í‘œí˜„
# - Token Type IDs : í•œ ìŒì˜ ë¬¸ì¥ ë˜ëŠ” ì§ˆë¬¸ ë‹µë³€ì— ëŒ€í•œ ë¶„ë¥˜ ì‹œ ì‚¬ìš©  
# - attention mask : `1`ì€ ì£¼ëª©í•´ì•¼ í•˜ëŠ” ê°’ì„ ë‚˜íƒ€ë‚´ê³  `0`ì€ íŒ¨ë”©ëœ ê°’ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  
# ```
# [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]
# ex) [CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]
# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]
# ```

# %% id="-OL3fgLvrXvH"
# í›ˆë ¨ ë°ì´í„°(X_train)ë¥¼ BERT ì…ë ¥ í˜•ì‹ì— ë§ê²Œ í† í¬ë‚˜ì´ì¦ˆ
# - truncation=True: ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ë¬¸ì¥ì€ ìë™ìœ¼ë¡œ ìë¦„
# - padding=True: ì§§ì€ ë¬¸ì¥ì€ ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° 0ìœ¼ë¡œ íŒ¨ë”©
train_encodings = tokenizer(X_train, truncation=True, padding=True)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°(X_test)ë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì¦ˆ
test_encodings = tokenizer(X_test, truncation=True, padding=True)

# %% colab={"base_uri": "https://localhost:8080/"} id="Flicg5UTgWAH" outputId="79b41c94-d032-489f-a2a6-5896796833d3"
# í† í¬ë‚˜ì´ì§•ëœ í›ˆë ¨ ë°ì´í„°ì˜ í‚¤ ëª©ë¡ í™•ì¸
# ì¼ë°˜ì ìœ¼ë¡œ 'input_ids', 'attention_mask', (ì„ íƒì ìœ¼ë¡œ 'token_type_ids')ê°€ í¬í•¨ë¨
print(type(train_encodings))

# %% colab={"base_uri": "https://localhost:8080/"} id="RWR8PRnbVsNg" outputId="bfa7b395-6f5e-4ae0-e2ca-4283ebca8d42"
print(train_encodings['input_ids'][0])
print(train_encodings['attention_mask'][0])
print(train_encodings['token_type_ids'][0])


# %% [markdown] id="F4demMyvho2E"
# ### Convert encodings to Tensors
#
# - ë ˆì´ë¸”ê³¼ ì¸ì½”ë”©ì„ Dataset ê°œì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. Pytorchë¥¼ ì´ìš©í•©ë‹ˆë‹¤.  
#
# - PyTorchì—ì„œ ì´ê²ƒì€ `torch.utils.data.Dataset` ê°ì²´ë¥¼ í•˜ê³  `__len__` ë° `__getitem__`ì„ êµ¬í˜„í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤.
#

# %% id="9B42CTCnrrEx"
# PyTorch Dataset í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ì—¬ IMDb ê°ì„± ë¶„ì„ìš© ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì •ì˜
class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels=None):
        # í† í¬ë‚˜ì´ì¦ˆëœ ì…ë ¥ (input_ids, attention_mask ë“±) ì €ì¥
        self.encodings = encodings
        # ì •ë‹µ ë ˆì´ë¸” (ì„ íƒì‚¬í•­)
        self.labels = labels

    def __getitem__(self, idx):
        # ì£¼ì–´ì§„ ì¸ë±ìŠ¤(idx)ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ì¶”ì¶œ
        # encodings ë”•ì…”ë„ˆë¦¬ì—ì„œ ê° í•­ëª©ë³„ë¡œ ê°™ì€ ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œí•˜ê³  í…ì„œë¡œ ë³€í™˜
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        # ë ˆì´ë¸”ì´ ìˆëŠ” ê²½ìš° í•¨ê»˜ ë°˜í™˜
        if self.labels:
            item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        # ë°ì´í„°ì…‹ì˜ ì „ì²´ ìƒ˜í”Œ ìˆ˜ ë°˜í™˜
        return len(self.encodings["input_ids"])

# í›ˆë ¨ìš© PyTorch Dataset ê°ì²´ ìƒì„±
train_dataset = IMDbDataset(train_encodings, y_train)

# í…ŒìŠ¤íŠ¸ìš© PyTorch Dataset ê°ì²´ ìƒì„±
test_dataset = IMDbDataset(test_encodings, y_test)

# %% [markdown] id="nXSPn9lCip6H"
# ì´ì œ ë°ì´í„° ì„¸íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë¯€ë¡œ ğŸ¤— `Trainer` ë˜ëŠ” ê¸°ë³¸ PyTorch/TensorFlowë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [training](https://huggingface.co/transformers/training.html)ì„ ì°¸ì¡°í•˜ì„¸ìš”.
#
# - Training warmup steps :  
#
#     - ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì„¤ì •ëœ ìˆ˜ì˜ í›ˆë ¨ ë‹¨ê³„(ì›Œë°ì—… ë‹¨ê³„)ì— ëŒ€í•´ ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì›Œë°ì—… ë‹¨ê³„ í›„ì— "ì¼ë°˜" í•™ìŠµë¥  ë˜ëŠ” í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë˜í•œ ì›Œë°ì—… ë‹¨ê³„ ìˆ˜ì— ë”°ë¼ í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# - weight_decay : ê°€ì¤‘ì¹˜ ê°ì‡ . L2 regularization

# %% colab={"base_uri": "https://localhost:8080/"} id="NH1dupK0rzfn" outputId="dfb9c7f2-4cae-4db8-b98e-c3c4db22cf09"
import os
os.environ["WANDB_DISABLED"] = "true"

training_args = TrainingArguments(
    output_dir='./results',               # ëª¨ë¸ ì¶œë ¥ ê²°ê³¼(ê°€ì¤‘ì¹˜ ë“±)ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
    num_train_epochs=2,                   # í•™ìŠµ ì „ì²´ epoch ìˆ˜
    per_device_train_batch_size=8,        # í•™ìŠµ ì‹œ ë””ë°”ì´ìŠ¤(GPU/CPU)ë‹¹ ë°°ì¹˜ í¬ê¸°
    per_device_eval_batch_size=16,        # í‰ê°€ ì‹œ ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°
    warmup_steps=500,                     # ëŸ¬ë‹ë ˆì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ìœ„í•œ ì›Œë°ì—… ìŠ¤í… ìˆ˜
    weight_decay=0.01,                    # ê°€ì¤‘ì¹˜ ê°ì‡ (ì •ê·œí™”) ê³„ìˆ˜
    logging_dir='./logs',                 # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
    logging_steps=100,                    # ëª‡ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ë¥¼ ì¶œë ¥í• ì§€ ì„¤ì •
    report_to=None,                       # wandb ë¹„í™œì„±í™”
    run_name='naver_movie_sentiment'      # ëª…ì‹œì  run_name ì„¤ì •
)

# %% [markdown] id="IHfVOJPSkOSi"
# ### model Train
# - Xet StorageëŠ” Hugging Faceì—ì„œ ë„ì…í•œ ê³ ì† ë²„ì „ ê´€ë¦¬ + ìŠ¤í† ë¦¬ì§€ ì‹œìŠ¤í…œìœ¼ë¡œ,
# ëª¨ë¸ê³¼ ë°ì´í„° íŒŒì¼ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ/ì—…ë¡œë“œ/ë²„ì „ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.  
# - wandbëŠ” Weights & Biasesì˜ ì•½ìë¡œ, ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ í”„ë¡œì íŠ¸ì˜ í•™ìŠµ ê³¼ì •ì„ ì‹œê°í™”Â·ì¶”ì Â·ê´€ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
# ëŒ€í‘œì ìœ¼ë¡œ Hugging Face Trainer, PyTorch, TensorFlow ë“±ê³¼ ì‰½ê²Œ ì—°ë™ë©ë‹ˆë‹¤.
#
# Colab ì—ì„œ ì•½ 17ë¶„ ì†Œìš”

# %% colab={"base_uri": "https://localhost:8080/", "height": 1000} id="PZvTrEcfr7k-" outputId="eea7d112-fabb-4f7f-b9f9-a976e1c31cbf"
import time

# ì‚¬ì „í•™ìŠµëœ ë‹¤êµ­ì–´ BERT ëª¨ë¸ ë¡œë“œ (ë¬¸ì¥ ë¶„ë¥˜ìš©ìœ¼ë¡œ headê°€ ë¶™ì–´ ìˆìŒ)
model = BertForSequenceClassification.from_pretrained(
    'bert-base-multilingual-cased',
    num_labels=2  # ê¸ì •/ë¶€ì • 2ê°œ í´ë˜ìŠ¤ ëª…ì‹œì  ì§€ì •
)

# Hugging Faceì˜ Trainer ê°ì²´ ìƒì„±
trainer = Trainer(
    model=model,                  # í•™ìŠµí•  ëª¨ë¸
    args=training_args,           # í•™ìŠµ ì„¤ì • (TrainingArguments ê°ì²´)
    train_dataset=train_dataset,  # í›ˆë ¨ ë°ì´í„°ì…‹
    eval_dataset=test_dataset     # í‰ê°€ ë°ì´í„°ì…‹
)

# í•™ìŠµ ì‹œì‘ ì‹œê°„ ê¸°ë¡
s = time.time()

# ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰
trainer.train()

# %% colab={"base_uri": "https://localhost:8080/"} id="DzpU6z9ELiYB" outputId="dbcb5541-f80d-4882-9c02-badac92fc801"
print("ê²½ê³¼ ì‹œê°„ : {:.2f}ë¶„".format((time.time() - s)/60))

# %% colab={"base_uri": "https://localhost:8080/", "height": 126} id="R534aDi3xD0s" outputId="d2244ac6-790f-4de0-89c7-ce348a9bf5b3"
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
# ë°˜í™˜ê°’ì—ëŠ” ì†ì‹¤(loss), ì •í™•ë„(accuracy) ë“±ì˜ í‰ê°€ ì§€í‘œê°€ í¬í•¨ë¨
trainer.evaluate(test_dataset)

# %% colab={"base_uri": "https://localhost:8080/", "height": 160} id="UyBmI1WcxKjG" outputId="98a329d0-8a46-412b-c7ef-ef489d67cb36"
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
# ì¶œë ¥ì€ ì˜ˆì¸¡ ê²°ê³¼(predictions), ì‹¤ì œ ì •ë‹µ(label_ids), í‰ê°€ ì§€í‘œ(metrics)ë¥¼ í¬í•¨í•œ ê°ì²´
prediction = trainer.predict(test_dataset)
prediction

# %% [markdown] id="Yb33pVW4H8u6"
# fine-tuned model ì€ logit ì„ return

# %% colab={"base_uri": "https://localhost:8080/"} id="mQ7XpU0oKBjG" outputId="06c8bd52-d7ad-4bdc-965c-ebcdcd0985cd"
# í˜„ì¬ Trainerì— í¬í•¨ëœ ëª¨ë¸ì—ì„œ ë¶„ë¥˜ê¸°(classifier) ì¸µ í™•ì¸
# ì´ ì¸µì€ BERT ì¶œë ¥(hidden state)ì„ ë°›ì•„ ìµœì¢… ë¶„ë¥˜ ê²°ê³¼ë¥¼ ê³„ì‚°í•˜ëŠ” ë ˆì´ì–´
trainer.model.classifier

# %% colab={"base_uri": "https://localhost:8080/"} id="9Qc5FtM8xn9A" outputId="98936510-8a25-4fc3-a47f-bc5056f5356e"
# ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ë¡œì§“(logits) ê°’ì„ í…ì„œë¡œ ë³€í™˜
# prediction[0]ì€ trainer.predict()ì˜ ê²°ê³¼ ì¤‘ 'predictions' (ë¡œì§“ ê°’)
y_logit = torch.tensor(prediction[0])

# ì²˜ìŒ 10ê°œ ìƒ˜í”Œì˜ ë¡œì§“ ì¶œë ¥
# ê° ìƒ˜í”Œë§ˆë‹¤ í´ë˜ìŠ¤ ìˆ˜ë§Œí¼ì˜ ì ìˆ˜(ì˜ˆ: 2-class ë¶„ë¥˜ë©´ [logit0, logit1])ê°€ ìˆìŒ
y_logit[:10]

# %% colab={"base_uri": "https://localhost:8080/"} id="fUVX_IhWxkxg" outputId="7751d34d-30e4-4a86-ab0a-e01e778c19d2"
# ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê° ìƒ˜í”Œì˜ í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ ê³„ì‚°
# dim=-1: ë§ˆì§€ë§‰ ì°¨ì›(í´ë˜ìŠ¤ ì°¨ì›) ê¸°ì¤€ìœ¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©
# argmax(axis=1): í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í´ë˜ìŠ¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ íƒ
# numpy(): PyTorch í…ì„œë¥¼ ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜
y_pred = F.softmax(y_logit, dim=-1).argmax(axis=1).numpy()

# ì˜ˆì¸¡ëœ ë ˆì´ë¸” ì¤‘ ì• 30ê°œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥
print(list(y_pred[:30]))

# ì‹¤ì œ ì •ë‹µ ë ˆì´ë¸”(y_test) ì¤‘ ì• 30ê°œë¥¼ ì¶œë ¥
print(y_test[:30])

# %% colab={"base_uri": "https://localhost:8080/"} id="cfCE06jQu5cI" outputId="244ff623-f46f-4bfa-97a3-9ab7da4bd641"
from sklearn.metrics import confusion_matrix, accuracy_score

# ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ì •ë‹µ ì‚¬ì´ì˜ ì •í™•ë„(accuracy)ë¥¼ ê³„ì‚°
print(accuracy_score(y_test, y_pred))

# í˜¼ë™ í–‰ë ¬(confusion matrix) ê³„ì‚°
# ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ë¹„êµí•˜ì—¬ ê° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‘œë¡œ ìš”ì•½
cm = confusion_matrix(y_test, y_pred)
cm

# %% colab={"base_uri": "https://localhost:8080/"} id="SIcfwq-cgaQD" outputId="bfff0645-7d39-4be8-9ac6-fe49c7f5f358"
# ì˜ˆì¸¡í•  ë¬¸ì¥
x = "ëˆì£¼ê³  ë³´ê¸°ì—ëŠ” ì•„ê¹Œìš´ ì˜í™” ã… ã… ..."
# x = "ë‚´ ì¸ìƒ ìµœê³  ëª…ì‘"

# 1. ì…ë ¥ í† í¬ë‚˜ì´ì¦ˆ
inputs = tokenizer([x], truncation=True, padding=True, return_tensors="pt")

# 2. ì…ë ¥ì„ GPUë¡œ ì´ë™í•˜ê³  ì˜ˆì¸¡
model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
inputs = {k: v.to(device) for k, v in inputs.items()}  # ëª¨ë“  ì…ë ¥ì„ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™

with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# 3. ì†Œí”„íŠ¸ë§¥ìŠ¤ â†’ í™•ë¥  â†’ argmax
probs = F.softmax(logits, dim=-1)
pred = torch.argmax(probs, dim=1).item()

# 4. ê²°ê³¼ ì¶œë ¥
print("ê¸ì •" if pred == 1 else "ë¶€ì •")

# %% [markdown] id="tvYSTPC5L2VY"
# # Next Step
# 20 ë§Œê°œ ì „ì²´ datasetìœ¼ë¡œ fine tuning
