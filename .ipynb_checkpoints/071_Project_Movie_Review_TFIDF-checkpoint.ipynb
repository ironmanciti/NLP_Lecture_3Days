{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f908cf",
   "metadata": {
    "id": "b4f908cf"
   },
   "source": [
    "# 프로젝트 A: TF-IDF 기반 영화 리뷰 감성 분석 시스템\n",
    "\n",
    "## 프로젝트 목표\n",
    "- 전통적 NLP 기법(TF-IDF + sklearn)을 활용한 감성 분석 시스템 구축\n",
    "- 네이버 영화평 데이터를 이용한 실전 감성 분석\n",
    "- Logistic Regression 분류기 학습 및 평가\n",
    "\n",
    "## 학습 내용\n",
    "1. 데이터 전처리 (텍스트 정제, 불용어 제거)\n",
    "2. TF-IDF 벡터화\n",
    "3. Logistic Regression 분류기 학습\n",
    "4. 성능 평가\n",
    "5. 키워드 분석 (긍정/부정 키워드 추출)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d016d1e",
   "metadata": {
    "id": "5d016d1e"
   },
   "source": [
    "---\n",
    "## 1. 데이터 준비 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd21f264",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd21f264",
    "lines_to_next_cell": 1,
    "outputId": "a610d878-0163-45f1-cec7-313491273823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.github.com/ironmanciti/Infran_NLP/master/data/naver_movie/ratings_train.txt\n",
      "\u001b[1m14628807/14628807\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://raw.github.com/ironmanciti/Infran_NLP/master/data/naver_movie/ratings_test.txt\n",
      "\u001b[1m4893335/4893335\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "================================================================================\n",
      "[데이터 로드 완료]\n",
      "================================================================================\n",
      "훈련 데이터: (150000, 3)\n",
      "테스트 데이터: (50000, 3)\n",
      "\n",
      "결측값 제거 후:\n",
      "훈련 데이터: (149995, 3)\n",
      "테스트 데이터: (49997, 3)\n",
      "\n",
      "샘플링 후:\n",
      "훈련 데이터: (10000, 3)\n",
      "테스트 데이터: (3000, 3)\n",
      "\n",
      "[레이블 분포]\n",
      "label\n",
      "0    5046\n",
      "1    4954\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 네이버 영화평 데이터 다운로드\n",
    "DATA_TRAIN_PATH = tf.keras.utils.get_file(\n",
    "    \"ratings_train.txt\",\n",
    "    \"https://raw.github.com/ironmanciti/Infran_NLP/master/data/naver_movie/ratings_train.txt\"\n",
    ")\n",
    "DATA_TEST_PATH = tf.keras.utils.get_file(\n",
    "    \"ratings_test.txt\",\n",
    "    \"https://raw.github.com/ironmanciti/Infran_NLP/master/data/naver_movie/ratings_test.txt\"\n",
    ")\n",
    "\n",
    "# 데이터 로드\n",
    "train_data = pd.read_csv(DATA_TRAIN_PATH, delimiter='\\t')\n",
    "test_data = pd.read_csv(DATA_TEST_PATH, delimiter='\\t')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"[데이터 로드 완료]\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"훈련 데이터: {train_data.shape}\")\n",
    "print(f\"테스트 데이터: {test_data.shape}\")\n",
    "\n",
    "# 결측값 제거\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "print(f\"\\n결측값 제거 후:\")\n",
    "print(f\"훈련 데이터: {train_data.shape}\")\n",
    "print(f\"테스트 데이터: {test_data.shape}\")\n",
    "\n",
    "# 데이터 샘플링 (처리 속도 향상을 위해)\n",
    "df_train = train_data.sample(n=10_000, random_state=1)\n",
    "df_test = test_data.sample(n=3_000, random_state=1)\n",
    "\n",
    "print(f\"\\n샘플링 후:\")\n",
    "print(f\"훈련 데이터: {df_train.shape}\")\n",
    "print(f\"테스트 데이터: {df_test.shape}\")\n",
    "\n",
    "# 레이블 분포 확인\n",
    "print(\"\\n[레이블 분포]\")\n",
    "print(df_train['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361ebf4",
   "metadata": {
    "id": "0361ebf4"
   },
   "source": [
    "---\n",
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22cc3c4",
   "metadata": {
    "id": "f22cc3c4"
   },
   "source": [
    "### 2.1 텍스트 정제 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c979316",
   "metadata": {
    "id": "5c979316",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def clean_text_basic(text):\n",
    "    \"\"\"\n",
    "    기본 텍스트 정제 함수\n",
    "    - HTML 태그 제거\n",
    "    - URL 제거\n",
    "    - 이메일 제거\n",
    "    - 전화번호 제거\n",
    "    - 특수문자 정규화\n",
    "    - 중복 공백 정리\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text)\n",
    "\n",
    "    # HTML 태그 제거\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # URL 제거\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "\n",
    "    # 이메일 제거\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # 전화번호 제거\n",
    "    text = re.sub(r'\\d{2,3}-\\d{3,4}-\\d{4}', '', text)\n",
    "\n",
    "    # 한글, 영문, 숫자만 유지 (구두점 제거)\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # 중복 공백 정리\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb1e35",
   "metadata": {
    "id": "73cb1e35"
   },
   "source": [
    "### 2.2 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e4f02ff",
   "metadata": {
    "id": "8e4f02ff",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 한국어 불용어 리스트\n",
    "korean_stopwords = [\n",
    "    '이', '가', '을', '를', '에', '의', '와', '과', '도', '로', '으로',\n",
    "    '은', '는', '에서', '에게', '께', '한테', '에게서', '한테서',\n",
    "    '이다', '있다', '되다', '하다', '되', '하', '것', '수', '등',\n",
    "    '그', '저', '이것', '그것', '저것', '그런', '이런', '저런',\n",
    "    '그리고', '그러나', '하지만', '또한', '또', '그래서', '그러므로'\n",
    "]\n",
    "\n",
    "def remove_stopwords(text, stopwords_list):\n",
    "    \"\"\"불용어 제거 함수\"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords_list]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51501b",
   "metadata": {
    "id": "6d51501b"
   },
   "source": [
    "### 2.3 전처리 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2b9b3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc2b9b3f",
    "outputId": "a1500498-f12a-4a5b-853f-71a9feb5c024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[데이터 전처리 진행 중...]\n",
      "================================================================================\n",
      "전처리 후 훈련 데이터: (9954, 4)\n",
      "전처리 후 테스트 데이터: (2986, 4)\n",
      "\n",
      "[전처리 결과 샘플]\n",
      "\n",
      "원본 1: (평점조절용 1) 애니인데 분위기가 좀 음산? 그로테스크하고, 캐릭터들이 무민가족 빼고 인...\n",
      "정제 1: 평점조절용 1 애니인데 분위기가 좀 음산 그로테스크하고 캐릭터들이 무민가족 빼고 인간인지 ...\n",
      "\n",
      "원본 2: 성우님들의 열연이 마음에 들었습니다...\n",
      "정제 2: 성우님들의 열연이 마음에 들었습니다...\n",
      "\n",
      "원본 3: 무지막지하게 지루함. 배우들이 아까움. 내용도 앞뒤없고 별로 들어오는 내용도 없고 답답함....\n",
      "정제 3: 무지막지하게 지루함 배우들이 아까움 내용도 앞뒤없고 별로 들어오는 내용도 없고 답답함 이야...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"[데이터 전처리 진행 중...]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 텍스트 정제\n",
    "df_train['cleaned_document'] = df_train['document'].apply(clean_text_basic)\n",
    "df_test['cleaned_document'] = df_test['document'].apply(clean_text_basic)\n",
    "\n",
    "# 불용어 제거\n",
    "df_train['cleaned_document'] = df_train['cleaned_document'].apply(\n",
    "    lambda x: remove_stopwords(x, korean_stopwords)\n",
    ")\n",
    "df_test['cleaned_document'] = df_test['cleaned_document'].apply(\n",
    "    lambda x: remove_stopwords(x, korean_stopwords)\n",
    ")\n",
    "\n",
    "# 빈 문자열 제거\n",
    "df_train = df_train[df_train['cleaned_document'].str.len() > 0]\n",
    "df_test = df_test[df_test['cleaned_document'].str.len() > 0]\n",
    "\n",
    "print(f\"전처리 후 훈련 데이터: {df_train.shape}\")\n",
    "print(f\"전처리 후 테스트 데이터: {df_test.shape}\")\n",
    "\n",
    "# 전처리 결과 샘플 확인\n",
    "print(\"\\n[전처리 결과 샘플]\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n원본 {i+1}: {df_train['document'].iloc[i][:50]}...\")\n",
    "    print(f\"정제 {i+1}: {df_train['cleaned_document'].iloc[i][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d0119",
   "metadata": {
    "id": "1c2d0119"
   },
   "source": [
    "---\n",
    "## 3. TF-IDF 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32eed0e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32eed0e5",
    "outputId": "f6a6af62-6c8c-4274-d1c2-ecb2ad99d91d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[TF-IDF 벡터화]\n",
      "================================================================================\n",
      "훈련 데이터 TF-IDF 벡터: (9954, 5000)\n",
      "테스트 데이터 TF-IDF 벡터: (2986, 5000)\n",
      "특성 수 (단어 수): 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"[TF-IDF 벡터화]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# TF-IDF 벡터화기 생성\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # 최대 특성 수 (처리 속도 향상)\n",
    "    ngram_range=(1, 2),  # 1-gram과 2-gram 사용\n",
    "    min_df=2,  # 최소 문서 빈도\n",
    "    max_df=0.95  # 최대 문서 빈도\n",
    ")\n",
    "\n",
    "# 훈련 데이터로 fit하고 transform\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(df_train['cleaned_document'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['cleaned_document'])\n",
    "\n",
    "# 레이블 추출\n",
    "y_train = df_train['label'].values\n",
    "y_test = df_test['label'].values\n",
    "\n",
    "print(f\"훈련 데이터 TF-IDF 벡터: {X_train_tfidf.shape}\")\n",
    "print(f\"테스트 데이터 TF-IDF 벡터: {X_test_tfidf.shape}\")\n",
    "print(f\"특성 수 (단어 수): {len(tfidf_vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861253d",
   "metadata": {
    "id": "1861253d"
   },
   "source": [
    "---\n",
    "## 4. 분류기 학습 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab436d3",
   "metadata": {
    "id": "7ab436d3"
   },
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f9cf068",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f9cf068",
    "outputId": "b9b6a83e-f0d8-466d-dc52-180c791e4e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7224\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = lr_model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"정확도: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd71ce0",
   "metadata": {
    "id": "ccd71ce0",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
