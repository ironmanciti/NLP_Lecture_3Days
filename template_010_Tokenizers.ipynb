{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3f9d7b",
   "metadata": {
    "id": "whjogPl1KL4-"
   },
   "source": [
    "# 037. 토크나이저 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoNLPy(한국어 형태소 분석기 패키지) 설치\n",
    "# SentencePiece(서브워드 토크나이저 도구)를 최신 버전으로 업그레이드 및 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20114174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746e8f3d",
   "metadata": {
    "id": "On8u7bpSjYci"
   },
   "source": [
    "# 1. Keras 기본 Tokenizer - rule-based\n",
    "- 공백 또는 구둣점으로 분리  \n",
    "- 영어 단어별로 띄어쓰기가 철저히 지켜지는 언어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 상위 100개의 단어로 구성된 Tokenizer 객체 생성 (OOV(Out-Of-Vocabulary) 토큰 설정)\n",
    "# 주어진 문장 리스트에 대해 토크나이저 학습 수행 (단어 인덱스 구축)\n",
    "# 구축된 단어 인덱스 사전 가져오기\n",
    "# 단어 인덱스 사전 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4bcb40",
   "metadata": {
    "id": "0TJA8onMkTiy"
   },
   "source": [
    "Keras의 rule base tokenizer로 한글을 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 상위 100개의 단어로 구성된 Tokenizer 객체 생성 (OOV(Out-Of-Vocabulary) 토큰 설정)\n",
    "# 주어진 한글 문장 리스트에 대해 토크나이저 학습 수행 (단어 인덱스 구축)\n",
    "# 구축된 단어 인덱스 사전 가져오기\n",
    "# 단어 인덱스 사전 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265889c1",
   "metadata": {
    "id": "TkQ9u94VjYcl"
   },
   "source": [
    "# 2. 단어 사전 기반 한국어 tokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okt 형태소 분석기 객체 생성\n",
    "# 형태소 분석 결과를 저장할 리스트 초기화\n",
    "# 주어진 한글 문장 리스트의 각 문장에 대해 반복\n",
    "    # 문장을 형태소 분석하여 결과를 리스트에 추가\n",
    "    # 형태소 분석 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae2895",
   "metadata": {
    "id": "MKy3rIq0kuLo"
   },
   "source": [
    "사전 기반 tokenize 후 Keras tokenizer 로 vocabulary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f57472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 상위 100개의 단어로 구성된 Tokenizer 객체 생성 (OOV(Out-Of-Vocabulary) 토큰 설정)\n",
    "# 형태소 분석된 문장 리스트에 대해 토크나이저 학습 수행 (단어 인덱스 구축)\n",
    "# 구축된 단어 인덱스 사전 가져오기\n",
    "# 단어 인덱스 사전 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c60a6f",
   "metadata": {
    "id": "6FRpIxColOLv"
   },
   "source": [
    "두 vocabulary 의 차이 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee6133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbf3efe7",
   "metadata": {
    "id": "MRtVftAuMmJ8"
   },
   "source": [
    "### 단, Okt 사전에 미등록된 단어의 경우 정확한 tokenizing 이 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 문장을 형태소 분석하여 품사 태깅 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2003b5",
   "metadata": {
    "id": "6Cq9CJhJMmJ9"
   },
   "source": [
    "예를 들어 `너무너무너무`와 `나카무라세이코`는 하나의 단어이지만, okt 사전에 등록되어 있지 않아 여러 개의 복합단어로 나뉘어집니다. 이러한 문제를 해결하기 위하여 형태소 분석기와 품사 판별기들은 사용자 사전 추가 기능을 제공합니다. 사용자 사전을 추가하여 모델의 vocabulary 를 풍부하게 만드는 것은 사용자의 몫입니다.\n",
    "\n",
    "1. okt 공식 문서를 참고해서 사용사 사전을 추가.\n",
    "2. okt를 패키징하고, konlpy에서 사용할 수 있도록 konlpy/java 경로에 jar 파일을 복사.\n",
    "3. 기존에 참고하고 있던 okt.jar 대신 새로운 okt.jar를 사용하도록 설정.\n",
    "4. konlpy 소스 경로를 import 해서 형태소 분석."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c47f3",
   "metadata": {
    "id": "9NSFEmKRMmKB"
   },
   "source": [
    "# 3. Google SentencePiece Tokenizer\n",
    "\n",
    "- NAVER Movie rating data 를 이용한 sentencepiece tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f83985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee24c31d",
   "metadata": {
    "id": "Vn-WA_c6MmKC"
   },
   "source": [
    "- pandas.read_csv에서 quoting = 3으로 설정해주면 인용구(따옴표)를 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769c9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540dfad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fe7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a687f7dd",
   "metadata": {
    "id": "4h4yXblIMmKE"
   },
   "source": [
    "## 학습을 위해 text 를 따로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa08c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'nsmc.txt' 파일을 쓰기 모드로 열기 (UTF-8 인코딩 사용)\n",
    "    # 훈련 데이터의 'document' 열에 있는 각 문장에 대해 반복\n",
    "            # 문장을 파일에 쓰고 새로운 줄 추가\n",
    "            # 쓰기 오류 발생 시 오류 메시지와 해당 문장 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write 가 잘 되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d222830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 파일 경로 설정\n",
    "# 어휘 사전의 최대 크기 설정\n",
    "# 모델 파일의 접두사 설정\n",
    "# 명령어 템플릿 정의\n",
    "# 템플릿에 변수 값을 포맷하여 명령어 문자열 생성\n",
    "# 생성된 명령어 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43bcbb",
   "metadata": {
    "id": "0IcsoxuvMmKF"
   },
   "source": [
    "### sentencepiece tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5790d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePieceTrainer를 사용하여 SentencePiece 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecdb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePieceProcessor 객체 생성\n",
    "# 학습된 SentencePiece 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2af372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터의 'document' 열에 있는 첫 세 개의 문장에 대해 반복\n",
    "    # 원본 문장 출력\n",
    "    # 문장을 SentencePiece 모델을 사용하여 토큰화하여 출력\n",
    "    # 문장을 SentencePiece 모델을 사용하여 인덱스 시퀀스로 변환하여 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1752b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 문장 리스트(sentences_K)에 있는 각 문장에 대해 반복\n",
    "    # 문장을 SentencePiece 모델을 사용하여 토큰화\n",
    "    # 문장을 SentencePiece 모델을 사용하여 인덱스 시퀀스로 변환\n",
    "    # 원본 문장 출력\n",
    "    # 토큰화된 결과 출력\n",
    "    # 인덱스 시퀀스 출력\n",
    "    # 각 문장 사이에 줄 바꿈 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34427c0b",
   "metadata": {
    "id": "JOm-NYpVjEfC"
   },
   "source": [
    "# OpenAI의 tiktoken을 사용한 tokenizer 개념 이해\n",
    "\n",
    "- tiktoken은 OpenAI에서 개발한 빠르고 효율적인 BPE(Byte Pair Encoding) 기반 토크나이저입니다.  \n",
    "- GPT 모델들이 사용하는 것과 동일한 토크나이징 방식을 제공합니다.\n",
    "\n",
    "```\n",
    "    \"cl100k_base\": \"GPT-4, GPT-3.5-turbo, text-embedding-ada-002에서 사용\",\n",
    "    \"p50k_base\": \"GPT-3, Codex에서 사용\",\n",
    "    \"r50k_base\": \"GPT-3, GPT-2에서 사용\"\n",
    "```\n",
    "\n",
    "- tiktoken은 BPE(Byte Pair Encoding) 방식을 사용합니다. 가장 자주 등장하는 문자 쌍을 하나의 토큰으로 병합하는 방식입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce59e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl100k_base 인코더 사용 (GPT-5와 동일)\n",
    "# 영어 문장 리스트 순회\n",
    "    # 텍스트를 토큰으로 변환 (문장을 토큰 ID 리스트로 인코딩)\n",
    "    # 토큰 ID를 다시 텍스트로 복원 (디코딩)\n",
    "    # 토큰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d43a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 텍스트를 토큰으로 변환\n",
    "    # 토큰을 다시 텍스트로 변환\n",
    "    # 토큰 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ad2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
